{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LANfactory","text":"<p>Lightweight python package to help with training LANs (Likelihood approximation networks). </p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The <code>LANfactory</code> package is a light-weight convenience package for training <code>likelihood approximation networks</code> (LANs) in torch (or keras),  starting from supplied training data.</p> <p>LANs, although more general in potential scope of applications, were conceived in the context of sequential sampling modeling to account for cognitive processes giving rise to choice and reaction time data in n-alternative forced choice experiments commonly encountered in the cognitive sciences.</p> <p>In this quick tutorial we will use the <code>ssms</code> package to generate our training data using such a sequential sampling model (SSM). The use is in no way bound to utilize the <code>ssms</code> package.</p>"},{"location":"#install","title":"Install","text":"<p>To install the <code>ssms</code> package type,</p> <p><code>pip install git+https://github.com/AlexanderFengler/ssm_simulators</code></p> <p>To install the <code>LANfactory</code> package type,</p> <p><code>pip install git+https://github.com/AlexanderFengler/LANfactory</code></p> <p>Necessary dependency should be installed automatically in the process.</p>"},{"location":"#basic-tutorial","title":"Basic Tutorial","text":"<pre><code># Load necessary packages\nimport ssms\nimport lanfactory \nimport os\nimport numpy as np\nfrom copy import deepcopy\nimport torch\n</code></pre>"},{"location":"#generate-training-data","title":"Generate Training Data","text":"<p>First we need to generate some training data. As mentioned above we will do so using the <code>ssms</code> python package, however without delving into a detailed explanation of this package. Please refer to the [basic ssms tutorial] (https://github.com/AlexanderFengler/ssm_simulators) in case you want to learn more.</p> <pre><code># MAKE CONFIGS\n\n# Initialize the generator config (for MLP LANs)\ngenerator_config = deepcopy(ssms.config.data_generator_config['lan']['mlp'])\n# Specify generative model (one from the list of included models mentioned above)\ngenerator_config['dgp_list'] = 'angle' \n# Specify number of parameter sets to simulate\ngenerator_config['n_parameter_sets'] = 100 \n# Specify how many samples a simulation run should entail\ngenerator_config['n_samples'] = 1000\n# Specify folder in which to save generated data\ngenerator_config['output_folder'] = 'data/lan_mlp/'\n\n# Make model config dict\nmodel_config = ssms.config.model_config['angle']\n</code></pre> <pre><code># MAKE DATA\n\nmy_dataset_generator = ssms.dataset_generators.data_generator(generator_config = generator_config,\n                                                              model_config = model_config)\n\ntraining_data = my_dataset_generator.generate_data_training_uniform(save = True)\n</code></pre> <pre><code>n_cpus used:  6\nchecking:  data/lan_mlp/\nsimulation round: 1  of 10\nsimulation round: 2  of 10\nsimulation round: 3  of 10\nsimulation round: 4  of 10\nsimulation round: 5  of 10\nsimulation round: 6  of 10\nsimulation round: 7  of 10\nsimulation round: 8  of 10\nsimulation round: 9  of 10\nsimulation round: 10  of 10\nWriting to file:  data/lan_mlp/training_data_0_nbins_0_n_1000/angle/training_data_angle_ef5b9e0eb76c11eca684acde48001122.pickle\n</code></pre>"},{"location":"#prepare-for-training","title":"Prepare for Training","text":"<p>Next we set up dataloaders for training with pytorch. The <code>LANfactory</code> uses custom dataloaders, taking into account particularities of the expected training data. Specifically, we expect to receive a bunch of training data files (the present example generates only one), where each file hosts a large number of training examples.  So we want to define a dataloader which spits out batches from data with a specific training data file, and keeps checking when to load in a new file.  The way this is implemented here, is via the <code>DatasetTorch</code> class in <code>lanfactory.trainers</code>, which inherits from <code>torch.utils.data.Dataset</code> and prespecifies a <code>batch_size</code>. Finally this is supplied to a <code>DataLoader</code>, for which we keep the <code>batch_size</code> argument at 0.</p> <p>The <code>DatasetTorch</code> class is then called as an iterator via the DataLoader and takes care of batching as well as file loading internally. </p> <p>You may choose your own way of defining the <code>DataLoader</code> classes, downstream you are simply expected to supply one.</p> <pre><code># MAKE DATALOADERS\n\n# List of datafiles (here only one)\nfolder_ = 'data/lan_mlp/training_data_0_nbins_0_n_1000/angle/'\nfile_list_ = [folder_ + file_ for file_ in os.listdir(folder_)]\n\n# Training dataset\ntorch_training_dataset = lanfactory.trainers.DatasetTorch(file_IDs = file_list_,\n                                                          batch_size = 128)\n\ntorch_training_dataloader = torch.utils.data.DataLoader(torch_training_dataset,\n                                                         shuffle = True,\n                                                         batch_size = None,\n                                                         num_workers = 1,\n                                                         pin_memory = True)\n\n# Validation dataset\ntorch_validation_dataset = lanfactory.trainers.DatasetTorch(file_IDs = file_list_,\n                                                          batch_size = 128)\n\ntorch_validation_dataloader = torch.utils.data.DataLoader(torch_validation_dataset,\n                                                          shuffle = True,\n                                                          batch_size = None,\n                                                          num_workers = 1,\n                                                          pin_memory = True)\n</code></pre> <p>Now we define two configuration dictionariers,</p> <ol> <li>The <code>network_config</code> dictionary defines the architecture and properties of the network</li> <li>The <code>train_config</code> dictionary defines properties concerning training hyperparameters</li> </ol> <p>Two examples (which we take as provided by the package, but which you can adjust according to your needs) are provided below.</p> <pre><code># SPECIFY NETWORK CONFIGS AND TRAINING CONFIGS\n\nnetwork_config = lanfactory.config.network_configs.network_config_mlp\n\nprint('Network config: ')\nprint(network_config)\n\ntrain_config = lanfactory.config.network_configs.train_config_mlp\n\nprint('Train config: ')\nprint(train_config)\n</code></pre> <pre><code>Network config: \n{'layer_types': ['dense', 'dense', 'dense'], 'layer_sizes': [100, 100, 1], 'activations': ['tanh', 'tanh', 'linear'], 'loss': ['huber'], 'callbacks': ['checkpoint', 'earlystopping', 'reducelr']}\nTrain config: \n{'batch_size': 128, 'n_epochs': 10, 'optimizer': 'adam', 'learning_rate': 0.002, 'loss': 'huber', 'save_history': True, 'metrics': [&lt;keras.losses.MeanSquaredError object at 0x12c403d30&gt;, &lt;keras.losses.Huber object at 0x12c1c78e0&gt;], 'callbacks': ['checkpoint', 'earlystopping', 'reducelr']}\n</code></pre> <p>We can now load a network, and save the configuration files for convenience.</p> <pre><code># LOAD NETWORK\nnet = lanfactory.trainers.TorchMLP(network_config = deepcopy(network_config),\n                                   input_shape = torch_training_dataset.input_dim,\n                                   save_folder = '/data/torch_models/',\n                                   generative_model_id = 'angle')\n\n# SAVE CONFIGS\nlanfactory.utils.save_configs(model_id = net.model_id + '_torch_',\n                              save_folder = 'data/torch_models/angle/', \n                              network_config = network_config, \n                              train_config = train_config, \n                              allow_abs_path_folder_generation = True)\n</code></pre> <p>To finally train the network we supply our network, the dataloaders and training config to the <code>ModelTrainerTorchMLP</code> class, from <code>lanfactory.trainers</code>.</p> <pre><code># TRAIN MODEL\nmodel_trainer.train_model(save_history = True,\n                          save_model = True,\n                          verbose = 0)\n</code></pre> <pre><code>Epoch took 0 / 10,  took 11.54538607597351 seconds\nepoch 0 / 10, validation_loss: 0.3431\nEpoch took 1 / 10,  took 13.032279014587402 seconds\nepoch 1 / 10, validation_loss: 0.2732\nEpoch took 2 / 10,  took 12.421074867248535 seconds\nepoch 2 / 10, validation_loss: 0.1941\nEpoch took 3 / 10,  took 12.097641229629517 seconds\nepoch 3 / 10, validation_loss: 0.2028\nEpoch took 4 / 10,  took 12.030233144760132 seconds\nepoch 4 / 10, validation_loss: 0.184\nEpoch took 5 / 10,  took 12.695374011993408 seconds\nepoch 5 / 10, validation_loss: 0.1433\nEpoch took 6 / 10,  took 12.177874326705933 seconds\nepoch 6 / 10, validation_loss: 0.1115\nEpoch took 7 / 10,  took 11.908828258514404 seconds\nepoch 7 / 10, validation_loss: 0.1084\nEpoch took 8 / 10,  took 12.066670179367065 seconds\nepoch 8 / 10, validation_loss: 0.0864\nEpoch took 9 / 10,  took 12.37562108039856 seconds\nepoch 9 / 10, validation_loss: 0.07484\nSaving training history\nSaving model state dict\nTraining finished successfully...\n</code></pre>"},{"location":"#load-model-for-inference-and-call","title":"Load Model for Inference and Call","text":"<p>The <code>LANfactory</code> provides some convenience functions to use networks for inference after training.  We can load a model using the <code>LoadTorchMLPInfer</code> class, which then allows us to run fast inference via either a direct call, which expects a <code>torch.tensor</code> as input, or the <code>predict_on_batch</code> method, which expects a <code>numpy.array</code>  of <code>dtype</code>, <code>np.float32</code>. </p> <pre><code>network_path_list = os.listdir('data/torch_models/angle')\nnetwork_file_path = ['data/torch_models/angle/' + file_ for file_ in network_path_list if 'state_dict' in file_][0]\n\nnetwork = lanfactory.trainers.LoadTorchMLPInfer(model_file_path = network_file_path,\n                                                network_config = network_config,\n                                                input_dim = torch_training_dataset.input_dim)\n</code></pre> <pre><code># Two ways to call the network\n\n# Direct call --&gt; need tensor input\ndirect_out = network(torch.from_numpy(np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype  = np.float32)))\nprint('direct call out: ', direct_out)\n\n# predict_on_batch method\npredict_on_batch_out = network.predict_on_batch(np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype  = np.float32))\nprint('predict_on_batch out: ', predict_on_batch_out)\n</code></pre> <pre><code>direct call out:  tensor([-16.4997])\npredict_on_batch out:  [-16.499687]\n</code></pre>"},{"location":"#a-peek-into-the-first-passage-distribution-computed-by-the-network","title":"A peek into the first passage distribution computed by the network","text":"<p>We can compare the learned likelihood function in our <code>network</code> with simulation data from the underlying generative model. For this purpose we recruit the <code>ssms</code> package again.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.DataFrame(np.zeros((2000, 7), dtype = np.float32), columns = ['v', 'a', 'z', 't', 'theta', 'rt', 'choice'])\ndata['v'] = 0.5\ndata['a'] = 0.75\ndata['z'] = 0.5\ndata['t'] = 0.2\ndata['theta'] = 0.1\ndata['rt'].iloc[:1000] = np.linspace(5, 0, 1000)\ndata['rt'].iloc[1000:] = np.linspace(0, 5, 1000)\ndata['choice'].iloc[:1000] = -1\ndata['choice'].iloc[1000:] = 1\n\n# Network predictions\npredict_on_batch_out = network.predict_on_batch(data.values.astype(np.float32))\n\n# Simulations\nfrom ssms.basic_simulators import simulator\nsim_out = simulator(model = 'angle', \n                    theta = data.values[0, :-2],\n                    n_samples = 2000)\n</code></pre> <pre><code># Plot network predictions\nplt.plot(data['rt'] * data['choice'], np.exp(predict_on_batch_out), color = 'black', label = 'network')\n\n# Plot simulations\nplt.hist(sim_out['rts'] * sim_out['choices'], bins = 30, histtype = 'step', label = 'simulations', color = 'blue', density  = True)\nplt.legend()\nplt.title('SSM likelihood')\nplt.xlabel('rt')\nplt.ylabel('likelihod')\n</code></pre> <pre><code>Text(0, 0.5, 'likelihod')\n</code></pre> <p></p>"},{"location":"#torchmlp-to-onnx-converter","title":"TorchMLP to ONNX Converter","text":"<p>The <code>transform_onnx.py</code> script converts a TorchMLP model to the ONNX format. It takes a network configuration file (in pickle format), a state dictionary file (Torch model weights), the size of the input tensor, and the desired output ONNX file path.</p>"},{"location":"#usage","title":"Usage","text":"<p><code>python onnx/transform_onnx.py &lt;network_config_file&gt; &lt;state_dict_file&gt; &lt;input_shape&gt; &lt;output_onnx_file&gt;</code></p> <p>Replace the placeholders with the appropriate values:</p> <ul> <li>: Path to the pickle file containing the network configuration. <li>: Path to the file containing the state dictionary of the model. <li>: The size of the input tensor for the model (integer). <li>: Path to the output ONNX file. <p>For example:</p> <p><pre><code>python onnx/transform_onnx.py '0d9f0e94175b11eca9e93cecef057438_lca_no_bias_4_torch__network_config.pickle' '0d9f0e94175b11eca9e93cecef057438_lca_no_bias_4_torch_state_dict.pt' 11 'lca_no_bias_4_torch.onnx'\n</code></pre> This onnx file can be used directly with the <code>HSSM</code> package. </p> <p>We hope this package may be helpful in case you attempt to train LANs for your own research.</p>"},{"location":"#end","title":"END","text":""},{"location":"api/config/","title":"config","text":""},{"location":"api/config/#lanfactory.config.network_configs","title":"lanfactory.config.network_configs","text":"<p>This Module defines simple examples for network and training configurations that serve as inputs to the training classes in the package.</p>"},{"location":"api/lanfactory/","title":"lanfactory","text":""},{"location":"api/lanfactory/#lanfactory.config","title":"lanfactory.config","text":""},{"location":"api/lanfactory/#lanfactory.config.network_configs","title":"network_configs","text":"<p>This Module defines simple examples for network and training configurations that serve as inputs to the training classes in the package.</p>"},{"location":"api/lanfactory/#lanfactory.onnx","title":"lanfactory.onnx","text":""},{"location":"api/lanfactory/#lanfactory.onnx.transform_onnx","title":"transform_onnx","text":""},{"location":"api/lanfactory/#lanfactory.onnx.transform_onnx.transform_to_onnx","title":"transform_to_onnx","text":"<pre><code>transform_to_onnx(network_config_file, state_dict_file, input_shape, output_onnx_file)\n</code></pre> <p>Transforms a TorchMLP model to ONNX format.</p> Arguments <pre><code>network_config_file (str):\n    Path to the pickle file containing the network configuration.\nstate_dict_file (str):\n    Path to the file containing the state dictionary of the model.\ninput_shape (int):\n    The size of the input tensor for the model.\noutput_onnx_file (str):\n    Path to the output ONNX file.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.onnx.transform_to_onnx","title":"transform_to_onnx","text":"<pre><code>transform_to_onnx(network_config_file, state_dict_file, input_shape, output_onnx_file)\n</code></pre> <p>Transforms a TorchMLP model to ONNX format.</p> Arguments <pre><code>network_config_file (str):\n    Path to the pickle file containing the network configuration.\nstate_dict_file (str):\n    Path to the file containing the state dictionary of the model.\ninput_shape (int):\n    The size of the input tensor for the model.\noutput_onnx_file (str):\n    Path to the output ONNX file.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers","title":"lanfactory.trainers","text":""},{"location":"api/lanfactory/#lanfactory.trainers.DatasetTorch","title":"DatasetTorch","text":"<pre><code>DatasetTorch(file_ids, batch_size=32, label_lower_bound=None, label_upper_bound=None, features_key='data', label_key='labels', out_framework='torch')\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset class for TorchMLP training.</p> Arguments <pre><code>file_ids (list):\n    List of paths to the data files.\nbatch_size (int):\n    Batch size.\nlabel_lower_bound (float):\n    Lower bound for the labels.\nlabel_upper_bound (float):\n    Upper bound for the labels.\nfeatures_key (str):\n    Key for the features in the data files.\nlabel_key (str):\n    Key for the labels in the data files.\nout_framework (str):\n    Output framework.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.LoadTorchMLP","title":"LoadTorchMLP","text":"<pre><code>LoadTorchMLP(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models.</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.LoadTorchMLPInfer","title":"LoadTorchMLPInfer","text":"<pre><code>LoadTorchMLPInfer(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models for inference. (This was originally useful directly for application in the HDDM toolbox).</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.LoadTorchMLPInfer.predict_on_batch","title":"predict_on_batch","text":"<pre><code>predict_on_batch(x=None)\n</code></pre> <p>Intended as function that computes trial wise log-likelihoods from a matrix input. To be used primarily through the HDDM toolbox.</p> Arguments <pre><code>x (numpy.ndarray(dtype=numpy.float32)):\n    Matrix which will be passed through the network.\n    LANs expect the matrix columns to follow a specific order.\n    When used in HDDM, x will be passed as follows.\n    The first few columns are trial wise model parameters\n    (order specified in the model_config file under the 'params' key).\n    The last two columns are filled with trial wise\n    reaction times and choices.\n    When not used via HDDM, no such restriction applies.\n</code></pre> Output <pre><code>numpy.ndarray(dtype = numpy.float32):\n    Output of the network. When called through HDDM,\n    this is expected as trial-wise log likelihoods\n    of a given generative model.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJax","title":"MLPJax","text":"<p>               Bases: <code>Module</code></p> <p>JaxMLP class.</p> Arguments <pre><code>layer_sizes (Sequence[int]):\n    Sequence of integers containing the sizes of the layers.\nactivations (Sequence[str]):\n    Sequence of strings containing the activation functions.\ntrain (bool):\n    Whether the model should be set to training mode or not.\ntrain_output_type (str):\n    The output type of the model during training.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJax.__call__","title":"__call__","text":"<pre><code>__call__(inputs)\n</code></pre> <p>Call function for the JaxMLP class. Performs forward pass through the network.</p> Arguments <pre><code>inputs (jax.numpy.ndarray):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    jax.numpy.ndarray:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJax.load_state_from_file","title":"load_state_from_file","text":"<pre><code>load_state_from_file(seed=42, input_dim=6, file_path=None)\n</code></pre> <p>Loads the state dictionary from a file.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nfile_path (str):\n    Path to the file containing the state dictionary.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The state dictionary.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJax.make_forward_partial","title":"make_forward_partial","text":"<pre><code>make_forward_partial(seed=42, input_dim=6, state=None, add_jitted=False)\n</code></pre> <p>Creates a partial function for the forward pass of the network.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nstate (flax.core.frozen_dict.FrozenDict):\n    The state dictionary (if not loaded from file).\nadd_jitted (bool):\n    Whether the partial function should be jitted or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    Callable:</code>           \u2013            <p>The partial function for the forward pass of the network.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJax.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Setup function for the JaxMLP class. Initializes the layers and activation functions.</p>"},{"location":"api/lanfactory/#lanfactory.trainers.MLPJaxFactory","title":"MLPJaxFactory","text":"<pre><code>MLPJaxFactory(network_config={}, train=True)\n</code></pre> <p>Factory function to create a MLPJax object.</p> Arguments <pre><code>network_config (dict):\n    Dictionary containing the network configuration.\ntrain (bool):\n    Whether the model should be trained or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    MLPJax class initialized with the correct network configuration.</code>           \u2013            </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerJaxMLP","title":"ModelTrainerJaxMLP","text":"<pre><code>ModelTrainerJaxMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=False, seed=None)\n</code></pre> Arguments <pre><code>train_config (dict):\n    Dictionary containing the training configuration.\nmodel (MLPJax):\n    The MLPJax model to be trained.\ntrain_dl (torch.utils.data.DataLoader):\n    The training data loader.\nvalid_dl (torch.utils.data.DataLoader):\n    The validation data loader.\nallow_abs_path_folder_generation (bool):\n    Whether the folder for the output files should be created or not.\npin_memory (bool):\n    Whether the data loader should pin memory or not.\nseed (int):\n    Seed for the random number generator.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    ModelTrainerJaxMLP:</code>           \u2013            <p>The ModelTrainerJaxMLP object.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerJaxMLP.create_train_state","title":"create_train_state","text":"<pre><code>create_train_state(rng)\n</code></pre> <p>Create initial train state</p>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerJaxMLP.run_epoch","title":"run_epoch","text":"<pre><code>run_epoch(state, train=True, verbose=1, epoch=0, max_epochs=0)\n</code></pre> <p>Run one epoch of training or validation</p> Arguments <pre><code>state (flax.core.frozen_dict.FrozenDict):\n    The state dictionary.\ntrain (bool):\n    Whether the model should is in training mode or not.\nverbose (int):\n    The verbosity level.\nepoch (int):\n    The current epoch.\nmax_epochs (int):\n    The maximum number of epochs.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    tuple (flax.core.frozen_dict.FrozenDict, float):</code>           \u2013            <p>The state dictionary and the mean epoch loss.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerJaxMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate JAXMLP model.</p> Arguments <pre><code>output_folder (str):\n    Path to the output folder.\noutput_file_id (str):\n    The file id.\nrun_id (str):\n    The run id.\nwandb_on (bool):\n    Whether to use wandb or not.\nwandb_project_id (str):\n    Project id for wandb.\nsave_history (bool):\n    Whether to save the training history or not.\nsave_model (bool):\n    Whether to save the model or not.\nsave_config (bool):\n    Whether to save the training configuration or not.\nsave_all (bool):\n    Whether to save all files or not.\nsave_data_details (bool):\n    Whether to save the data details or not.\nverbose (int):\n    The verbosity level.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The final state dictionary (model state).</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerTorchMLP","title":"ModelTrainerTorchMLP","text":"<pre><code>ModelTrainerTorchMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=True, seed=None)\n</code></pre> <pre><code>train_config (dict):\n    Training configuration.\nmodel (TorchMLP):\n    TorchMLP model.\ntrain_dl (DatasetTorch):\n    Training dataloader.\nvalid_dl (DatasetTorch):\n    Validation dataloader.\nallow_abs_path_folder_generation (bool):\n    Whether to allow absolute path folder generation.\npin_memory (bool):\n    Whether to pin memory (dataloader). Can affect speed.\nseed (int):\n    Random seed.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.ModelTrainerTorchMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_onnx=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate the model.</p> Arguments <pre><code>output_folder (str):\n    Output folder.\noutput_file_id (str):\n    Output file ID.\nrun_id (str):\n    Run ID.\nwandb_on (bool):\n    Whether to use wandb.\nwandb_project_id (str):\n    Wandb project ID.\nsave_history (bool):\n    Whether to save the training history.\nsave_model (bool):\n    Whether to save the model.\nsave_config (bool):\n    Whether to save the training configuration.\nsave_onnx (bool):\n    Whether to save the model to ONNX format.\nsave_all (bool):\n    Whether to save all.\nsave_data_details (bool):\n    Whether to save the data details.\nverbose (int):\n    Verbosity level.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.TorchMLP","title":"TorchMLP","text":"<pre><code>TorchMLP(network_config=None, input_shape=10, network_type=None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>TorchMLP class.</p> Arguments <pre><code>network_config (dict):\n    Network configuration.\ninput_shape (int):\n    Input shape.\nnetwork_type (str):\n    Network type.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.TorchMLP.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through network.</p> Arguments <pre><code>x (torch.Tensor):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    torch.Tensor:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp","title":"jax_mlp","text":""},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJax","title":"MLPJax","text":"<p>               Bases: <code>Module</code></p> <p>JaxMLP class.</p> Arguments <pre><code>layer_sizes (Sequence[int]):\n    Sequence of integers containing the sizes of the layers.\nactivations (Sequence[str]):\n    Sequence of strings containing the activation functions.\ntrain (bool):\n    Whether the model should be set to training mode or not.\ntrain_output_type (str):\n    The output type of the model during training.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJax.__call__","title":"__call__","text":"<pre><code>__call__(inputs)\n</code></pre> <p>Call function for the JaxMLP class. Performs forward pass through the network.</p> Arguments <pre><code>inputs (jax.numpy.ndarray):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    jax.numpy.ndarray:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJax.load_state_from_file","title":"load_state_from_file","text":"<pre><code>load_state_from_file(seed=42, input_dim=6, file_path=None)\n</code></pre> <p>Loads the state dictionary from a file.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nfile_path (str):\n    Path to the file containing the state dictionary.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The state dictionary.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJax.make_forward_partial","title":"make_forward_partial","text":"<pre><code>make_forward_partial(seed=42, input_dim=6, state=None, add_jitted=False)\n</code></pre> <p>Creates a partial function for the forward pass of the network.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nstate (flax.core.frozen_dict.FrozenDict):\n    The state dictionary (if not loaded from file).\nadd_jitted (bool):\n    Whether the partial function should be jitted or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    Callable:</code>           \u2013            <p>The partial function for the forward pass of the network.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJax.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Setup function for the JaxMLP class. Initializes the layers and activation functions.</p>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.MLPJaxFactory","title":"MLPJaxFactory","text":"<pre><code>MLPJaxFactory(network_config={}, train=True)\n</code></pre> <p>Factory function to create a MLPJax object.</p> Arguments <pre><code>network_config (dict):\n    Dictionary containing the network configuration.\ntrain (bool):\n    Whether the model should be trained or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    MLPJax class initialized with the correct network configuration.</code>           \u2013            </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP","title":"ModelTrainerJaxMLP","text":"<pre><code>ModelTrainerJaxMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=False, seed=None)\n</code></pre> Arguments <pre><code>train_config (dict):\n    Dictionary containing the training configuration.\nmodel (MLPJax):\n    The MLPJax model to be trained.\ntrain_dl (torch.utils.data.DataLoader):\n    The training data loader.\nvalid_dl (torch.utils.data.DataLoader):\n    The validation data loader.\nallow_abs_path_folder_generation (bool):\n    Whether the folder for the output files should be created or not.\npin_memory (bool):\n    Whether the data loader should pin memory or not.\nseed (int):\n    Seed for the random number generator.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    ModelTrainerJaxMLP:</code>           \u2013            <p>The ModelTrainerJaxMLP object.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.create_train_state","title":"create_train_state","text":"<pre><code>create_train_state(rng)\n</code></pre> <p>Create initial train state</p>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.run_epoch","title":"run_epoch","text":"<pre><code>run_epoch(state, train=True, verbose=1, epoch=0, max_epochs=0)\n</code></pre> <p>Run one epoch of training or validation</p> Arguments <pre><code>state (flax.core.frozen_dict.FrozenDict):\n    The state dictionary.\ntrain (bool):\n    Whether the model should is in training mode or not.\nverbose (int):\n    The verbosity level.\nepoch (int):\n    The current epoch.\nmax_epochs (int):\n    The maximum number of epochs.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    tuple (flax.core.frozen_dict.FrozenDict, float):</code>           \u2013            <p>The state dictionary and the mean epoch loss.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate JAXMLP model.</p> Arguments <pre><code>output_folder (str):\n    Path to the output folder.\noutput_file_id (str):\n    The file id.\nrun_id (str):\n    The run id.\nwandb_on (bool):\n    Whether to use wandb or not.\nwandb_project_id (str):\n    Project id for wandb.\nsave_history (bool):\n    Whether to save the training history or not.\nsave_model (bool):\n    Whether to save the model or not.\nsave_config (bool):\n    Whether to save the training configuration or not.\nsave_all (bool):\n    Whether to save all files or not.\nsave_data_details (bool):\n    Whether to save the data details or not.\nverbose (int):\n    The verbosity level.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The final state dictionary (model state).</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp","title":"torch_mlp","text":""},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.DatasetTorch","title":"DatasetTorch","text":"<pre><code>DatasetTorch(file_ids, batch_size=32, label_lower_bound=None, label_upper_bound=None, features_key='data', label_key='labels', out_framework='torch')\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset class for TorchMLP training.</p> Arguments <pre><code>file_ids (list):\n    List of paths to the data files.\nbatch_size (int):\n    Batch size.\nlabel_lower_bound (float):\n    Lower bound for the labels.\nlabel_upper_bound (float):\n    Upper bound for the labels.\nfeatures_key (str):\n    Key for the features in the data files.\nlabel_key (str):\n    Key for the labels in the data files.\nout_framework (str):\n    Output framework.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.LoadTorchMLP","title":"LoadTorchMLP","text":"<pre><code>LoadTorchMLP(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models.</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.LoadTorchMLPInfer","title":"LoadTorchMLPInfer","text":"<pre><code>LoadTorchMLPInfer(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models for inference. (This was originally useful directly for application in the HDDM toolbox).</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.LoadTorchMLPInfer.predict_on_batch","title":"predict_on_batch","text":"<pre><code>predict_on_batch(x=None)\n</code></pre> <p>Intended as function that computes trial wise log-likelihoods from a matrix input. To be used primarily through the HDDM toolbox.</p> Arguments <pre><code>x (numpy.ndarray(dtype=numpy.float32)):\n    Matrix which will be passed through the network.\n    LANs expect the matrix columns to follow a specific order.\n    When used in HDDM, x will be passed as follows.\n    The first few columns are trial wise model parameters\n    (order specified in the model_config file under the 'params' key).\n    The last two columns are filled with trial wise\n    reaction times and choices.\n    When not used via HDDM, no such restriction applies.\n</code></pre> Output <pre><code>numpy.ndarray(dtype = numpy.float32):\n    Output of the network. When called through HDDM,\n    this is expected as trial-wise log likelihoods\n    of a given generative model.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.ModelTrainerTorchMLP","title":"ModelTrainerTorchMLP","text":"<pre><code>ModelTrainerTorchMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=True, seed=None)\n</code></pre> <pre><code>train_config (dict):\n    Training configuration.\nmodel (TorchMLP):\n    TorchMLP model.\ntrain_dl (DatasetTorch):\n    Training dataloader.\nvalid_dl (DatasetTorch):\n    Validation dataloader.\nallow_abs_path_folder_generation (bool):\n    Whether to allow absolute path folder generation.\npin_memory (bool):\n    Whether to pin memory (dataloader). Can affect speed.\nseed (int):\n    Random seed.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.ModelTrainerTorchMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_onnx=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate the model.</p> Arguments <pre><code>output_folder (str):\n    Output folder.\noutput_file_id (str):\n    Output file ID.\nrun_id (str):\n    Run ID.\nwandb_on (bool):\n    Whether to use wandb.\nwandb_project_id (str):\n    Wandb project ID.\nsave_history (bool):\n    Whether to save the training history.\nsave_model (bool):\n    Whether to save the model.\nsave_config (bool):\n    Whether to save the training configuration.\nsave_onnx (bool):\n    Whether to save the model to ONNX format.\nsave_all (bool):\n    Whether to save all.\nsave_data_details (bool):\n    Whether to save the data details.\nverbose (int):\n    Verbosity level.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.TorchMLP","title":"TorchMLP","text":"<pre><code>TorchMLP(network_config=None, input_shape=10, network_type=None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>TorchMLP class.</p> Arguments <pre><code>network_config (dict):\n    Network configuration.\ninput_shape (int):\n    Input shape.\nnetwork_type (str):\n    Network type.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.trainers.torch_mlp.TorchMLP.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through network.</p> Arguments <pre><code>x (torch.Tensor):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    torch.Tensor:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/lanfactory/#lanfactory.utils","title":"lanfactory.utils","text":""},{"location":"api/lanfactory/#lanfactory.utils.save_configs","title":"save_configs","text":"<pre><code>save_configs(model_id=None, save_folder=None, network_config=None, train_config=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to save the network and training configurations to a folder.</p> Arguments <pre><code>model_id (str):\n    The id of the model.\nsave_folder (str):\n    The folder to save the configurations to.\nnetwork_config (dict):\n    The network configuration dictionary.\ntrain_config (dict):\n    The training configuration dictionary.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.utils.try_gen_folder","title":"try_gen_folder","text":"<pre><code>try_gen_folder(folder=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to generate a folder from a string. If the folder already exists, it will not be generated.</p> Arguments <pre><code>folder (str):\n    The folder string to generate.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.utils.util_funs","title":"util_funs","text":""},{"location":"api/lanfactory/#lanfactory.utils.util_funs.save_configs","title":"save_configs","text":"<pre><code>save_configs(model_id=None, save_folder=None, network_config=None, train_config=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to save the network and training configurations to a folder.</p> Arguments <pre><code>model_id (str):\n    The id of the model.\nsave_folder (str):\n    The folder to save the configurations to.\nnetwork_config (dict):\n    The network configuration dictionary.\ntrain_config (dict):\n    The training configuration dictionary.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/lanfactory/#lanfactory.utils.util_funs.try_gen_folder","title":"try_gen_folder","text":"<pre><code>try_gen_folder(folder=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to generate a folder from a string. If the folder already exists, it will not be generated.</p> Arguments <pre><code>folder (str):\n    The folder string to generate.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/onnx/","title":"onnx","text":""},{"location":"api/onnx/#lanfactory.onnx.transform_onnx","title":"lanfactory.onnx.transform_onnx","text":""},{"location":"api/onnx/#lanfactory.onnx.transform_onnx.transform_to_onnx","title":"transform_to_onnx","text":"<pre><code>transform_to_onnx(network_config_file, state_dict_file, input_shape, output_onnx_file)\n</code></pre> <p>Transforms a TorchMLP model to ONNX format.</p> Arguments <pre><code>network_config_file (str):\n    Path to the pickle file containing the network configuration.\nstate_dict_file (str):\n    Path to the file containing the state dictionary of the model.\ninput_shape (int):\n    The size of the input tensor for the model.\noutput_onnx_file (str):\n    Path to the output ONNX file.\n</code></pre>"},{"location":"api/onnx/#lanfactory.onnx.transform_to_onnx","title":"lanfactory.onnx.transform_to_onnx","text":"<pre><code>transform_to_onnx(network_config_file, state_dict_file, input_shape, output_onnx_file)\n</code></pre> <p>Transforms a TorchMLP model to ONNX format.</p> Arguments <pre><code>network_config_file (str):\n    Path to the pickle file containing the network configuration.\nstate_dict_file (str):\n    Path to the file containing the state dictionary of the model.\ninput_shape (int):\n    The size of the input tensor for the model.\noutput_onnx_file (str):\n    Path to the output ONNX file.\n</code></pre>"},{"location":"api/trainers/","title":"trainers","text":""},{"location":"api/trainers/#lanfactory.trainers.DatasetTorch","title":"lanfactory.trainers.DatasetTorch","text":"<pre><code>DatasetTorch(file_ids, batch_size=32, label_lower_bound=None, label_upper_bound=None, features_key='data', label_key='labels', out_framework='torch')\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset class for TorchMLP training.</p> Arguments <pre><code>file_ids (list):\n    List of paths to the data files.\nbatch_size (int):\n    Batch size.\nlabel_lower_bound (float):\n    Lower bound for the labels.\nlabel_upper_bound (float):\n    Upper bound for the labels.\nfeatures_key (str):\n    Key for the features in the data files.\nlabel_key (str):\n    Key for the labels in the data files.\nout_framework (str):\n    Output framework.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.LoadTorchMLP","title":"lanfactory.trainers.LoadTorchMLP","text":"<pre><code>LoadTorchMLP(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models.</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.LoadTorchMLPInfer","title":"lanfactory.trainers.LoadTorchMLPInfer","text":"<pre><code>LoadTorchMLPInfer(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models for inference. (This was originally useful directly for application in the HDDM toolbox).</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.LoadTorchMLPInfer.predict_on_batch","title":"predict_on_batch","text":"<pre><code>predict_on_batch(x=None)\n</code></pre> <p>Intended as function that computes trial wise log-likelihoods from a matrix input. To be used primarily through the HDDM toolbox.</p> Arguments <pre><code>x (numpy.ndarray(dtype=numpy.float32)):\n    Matrix which will be passed through the network.\n    LANs expect the matrix columns to follow a specific order.\n    When used in HDDM, x will be passed as follows.\n    The first few columns are trial wise model parameters\n    (order specified in the model_config file under the 'params' key).\n    The last two columns are filled with trial wise\n    reaction times and choices.\n    When not used via HDDM, no such restriction applies.\n</code></pre> Output <pre><code>numpy.ndarray(dtype = numpy.float32):\n    Output of the network. When called through HDDM,\n    this is expected as trial-wise log likelihoods\n    of a given generative model.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.MLPJax","title":"lanfactory.trainers.MLPJax","text":"<p>               Bases: <code>Module</code></p> <p>JaxMLP class.</p> Arguments <pre><code>layer_sizes (Sequence[int]):\n    Sequence of integers containing the sizes of the layers.\nactivations (Sequence[str]):\n    Sequence of strings containing the activation functions.\ntrain (bool):\n    Whether the model should be set to training mode or not.\ntrain_output_type (str):\n    The output type of the model during training.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.MLPJax.__call__","title":"__call__","text":"<pre><code>__call__(inputs)\n</code></pre> <p>Call function for the JaxMLP class. Performs forward pass through the network.</p> Arguments <pre><code>inputs (jax.numpy.ndarray):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    jax.numpy.ndarray:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.MLPJax.load_state_from_file","title":"load_state_from_file","text":"<pre><code>load_state_from_file(seed=42, input_dim=6, file_path=None)\n</code></pre> <p>Loads the state dictionary from a file.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nfile_path (str):\n    Path to the file containing the state dictionary.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The state dictionary.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.MLPJax.make_forward_partial","title":"make_forward_partial","text":"<pre><code>make_forward_partial(seed=42, input_dim=6, state=None, add_jitted=False)\n</code></pre> <p>Creates a partial function for the forward pass of the network.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nstate (flax.core.frozen_dict.FrozenDict):\n    The state dictionary (if not loaded from file).\nadd_jitted (bool):\n    Whether the partial function should be jitted or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    Callable:</code>           \u2013            <p>The partial function for the forward pass of the network.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.MLPJax.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Setup function for the JaxMLP class. Initializes the layers and activation functions.</p>"},{"location":"api/trainers/#lanfactory.trainers.MLPJaxFactory","title":"lanfactory.trainers.MLPJaxFactory","text":"<pre><code>MLPJaxFactory(network_config={}, train=True)\n</code></pre> <p>Factory function to create a MLPJax object.</p> Arguments <pre><code>network_config (dict):\n    Dictionary containing the network configuration.\ntrain (bool):\n    Whether the model should be trained or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    MLPJax class initialized with the correct network configuration.</code>           \u2013            </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerJaxMLP","title":"lanfactory.trainers.ModelTrainerJaxMLP","text":"<pre><code>ModelTrainerJaxMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=False, seed=None)\n</code></pre> Arguments <pre><code>train_config (dict):\n    Dictionary containing the training configuration.\nmodel (MLPJax):\n    The MLPJax model to be trained.\ntrain_dl (torch.utils.data.DataLoader):\n    The training data loader.\nvalid_dl (torch.utils.data.DataLoader):\n    The validation data loader.\nallow_abs_path_folder_generation (bool):\n    Whether the folder for the output files should be created or not.\npin_memory (bool):\n    Whether the data loader should pin memory or not.\nseed (int):\n    Seed for the random number generator.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    ModelTrainerJaxMLP:</code>           \u2013            <p>The ModelTrainerJaxMLP object.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerJaxMLP.create_train_state","title":"create_train_state","text":"<pre><code>create_train_state(rng)\n</code></pre> <p>Create initial train state</p>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerJaxMLP.run_epoch","title":"run_epoch","text":"<pre><code>run_epoch(state, train=True, verbose=1, epoch=0, max_epochs=0)\n</code></pre> <p>Run one epoch of training or validation</p> Arguments <pre><code>state (flax.core.frozen_dict.FrozenDict):\n    The state dictionary.\ntrain (bool):\n    Whether the model should is in training mode or not.\nverbose (int):\n    The verbosity level.\nepoch (int):\n    The current epoch.\nmax_epochs (int):\n    The maximum number of epochs.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    tuple (flax.core.frozen_dict.FrozenDict, float):</code>           \u2013            <p>The state dictionary and the mean epoch loss.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerJaxMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate JAXMLP model.</p> Arguments <pre><code>output_folder (str):\n    Path to the output folder.\noutput_file_id (str):\n    The file id.\nrun_id (str):\n    The run id.\nwandb_on (bool):\n    Whether to use wandb or not.\nwandb_project_id (str):\n    Project id for wandb.\nsave_history (bool):\n    Whether to save the training history or not.\nsave_model (bool):\n    Whether to save the model or not.\nsave_config (bool):\n    Whether to save the training configuration or not.\nsave_all (bool):\n    Whether to save all files or not.\nsave_data_details (bool):\n    Whether to save the data details or not.\nverbose (int):\n    The verbosity level.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The final state dictionary (model state).</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerTorchMLP","title":"lanfactory.trainers.ModelTrainerTorchMLP","text":"<pre><code>ModelTrainerTorchMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=True, seed=None)\n</code></pre> <pre><code>train_config (dict):\n    Training configuration.\nmodel (TorchMLP):\n    TorchMLP model.\ntrain_dl (DatasetTorch):\n    Training dataloader.\nvalid_dl (DatasetTorch):\n    Validation dataloader.\nallow_abs_path_folder_generation (bool):\n    Whether to allow absolute path folder generation.\npin_memory (bool):\n    Whether to pin memory (dataloader). Can affect speed.\nseed (int):\n    Random seed.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.ModelTrainerTorchMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_onnx=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate the model.</p> Arguments <pre><code>output_folder (str):\n    Output folder.\noutput_file_id (str):\n    Output file ID.\nrun_id (str):\n    Run ID.\nwandb_on (bool):\n    Whether to use wandb.\nwandb_project_id (str):\n    Wandb project ID.\nsave_history (bool):\n    Whether to save the training history.\nsave_model (bool):\n    Whether to save the model.\nsave_config (bool):\n    Whether to save the training configuration.\nsave_onnx (bool):\n    Whether to save the model to ONNX format.\nsave_all (bool):\n    Whether to save all.\nsave_data_details (bool):\n    Whether to save the data details.\nverbose (int):\n    Verbosity level.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.TorchMLP","title":"lanfactory.trainers.TorchMLP","text":"<pre><code>TorchMLP(network_config=None, input_shape=10, network_type=None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>TorchMLP class.</p> Arguments <pre><code>network_config (dict):\n    Network configuration.\ninput_shape (int):\n    Input shape.\nnetwork_type (str):\n    Network type.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.TorchMLP.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through network.</p> Arguments <pre><code>x (torch.Tensor):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    torch.Tensor:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp","title":"lanfactory.trainers.jax_mlp","text":""},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJax","title":"MLPJax","text":"<p>               Bases: <code>Module</code></p> <p>JaxMLP class.</p> Arguments <pre><code>layer_sizes (Sequence[int]):\n    Sequence of integers containing the sizes of the layers.\nactivations (Sequence[str]):\n    Sequence of strings containing the activation functions.\ntrain (bool):\n    Whether the model should be set to training mode or not.\ntrain_output_type (str):\n    The output type of the model during training.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJax.__call__","title":"__call__","text":"<pre><code>__call__(inputs)\n</code></pre> <p>Call function for the JaxMLP class. Performs forward pass through the network.</p> Arguments <pre><code>inputs (jax.numpy.ndarray):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    jax.numpy.ndarray:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJax.load_state_from_file","title":"load_state_from_file","text":"<pre><code>load_state_from_file(seed=42, input_dim=6, file_path=None)\n</code></pre> <p>Loads the state dictionary from a file.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nfile_path (str):\n    Path to the file containing the state dictionary.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The state dictionary.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJax.make_forward_partial","title":"make_forward_partial","text":"<pre><code>make_forward_partial(seed=42, input_dim=6, state=None, add_jitted=False)\n</code></pre> <p>Creates a partial function for the forward pass of the network.</p> Arguments <pre><code>seed (int):\n    Seed for the random number generator.\ninput_dim (int):\n    Dimension of the input tensor.\nstate (flax.core.frozen_dict.FrozenDict):\n    The state dictionary (if not loaded from file).\nadd_jitted (bool):\n    Whether the partial function should be jitted or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    Callable:</code>           \u2013            <p>The partial function for the forward pass of the network.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJax.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Setup function for the JaxMLP class. Initializes the layers and activation functions.</p>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.MLPJaxFactory","title":"MLPJaxFactory","text":"<pre><code>MLPJaxFactory(network_config={}, train=True)\n</code></pre> <p>Factory function to create a MLPJax object.</p> Arguments <pre><code>network_config (dict):\n    Dictionary containing the network configuration.\ntrain (bool):\n    Whether the model should be trained or not.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    MLPJax class initialized with the correct network configuration.</code>           \u2013            </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP","title":"ModelTrainerJaxMLP","text":"<pre><code>ModelTrainerJaxMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=False, seed=None)\n</code></pre> Arguments <pre><code>train_config (dict):\n    Dictionary containing the training configuration.\nmodel (MLPJax):\n    The MLPJax model to be trained.\ntrain_dl (torch.utils.data.DataLoader):\n    The training data loader.\nvalid_dl (torch.utils.data.DataLoader):\n    The validation data loader.\nallow_abs_path_folder_generation (bool):\n    Whether the folder for the output files should be created or not.\npin_memory (bool):\n    Whether the data loader should pin memory or not.\nseed (int):\n    Seed for the random number generator.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    ModelTrainerJaxMLP:</code>           \u2013            <p>The ModelTrainerJaxMLP object.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.create_train_state","title":"create_train_state","text":"<pre><code>create_train_state(rng)\n</code></pre> <p>Create initial train state</p>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.run_epoch","title":"run_epoch","text":"<pre><code>run_epoch(state, train=True, verbose=1, epoch=0, max_epochs=0)\n</code></pre> <p>Run one epoch of training or validation</p> Arguments <pre><code>state (flax.core.frozen_dict.FrozenDict):\n    The state dictionary.\ntrain (bool):\n    Whether the model should is in training mode or not.\nverbose (int):\n    The verbosity level.\nepoch (int):\n    The current epoch.\nmax_epochs (int):\n    The maximum number of epochs.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    tuple (flax.core.frozen_dict.FrozenDict, float):</code>           \u2013            <p>The state dictionary and the mean epoch loss.</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.jax_mlp.ModelTrainerJaxMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate JAXMLP model.</p> Arguments <pre><code>output_folder (str):\n    Path to the output folder.\noutput_file_id (str):\n    The file id.\nrun_id (str):\n    The run id.\nwandb_on (bool):\n    Whether to use wandb or not.\nwandb_project_id (str):\n    Project id for wandb.\nsave_history (bool):\n    Whether to save the training history or not.\nsave_model (bool):\n    Whether to save the model or not.\nsave_config (bool):\n    Whether to save the training configuration or not.\nsave_all (bool):\n    Whether to save all files or not.\nsave_data_details (bool):\n    Whether to save the data details or not.\nverbose (int):\n    The verbosity level.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    flax.core.frozen_dict.FrozenDict:</code>           \u2013            <p>The final state dictionary (model state).</p> </li> </ul>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp","title":"lanfactory.trainers.torch_mlp","text":""},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.DatasetTorch","title":"DatasetTorch","text":"<pre><code>DatasetTorch(file_ids, batch_size=32, label_lower_bound=None, label_upper_bound=None, features_key='data', label_key='labels', out_framework='torch')\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset class for TorchMLP training.</p> Arguments <pre><code>file_ids (list):\n    List of paths to the data files.\nbatch_size (int):\n    Batch size.\nlabel_lower_bound (float):\n    Lower bound for the labels.\nlabel_upper_bound (float):\n    Upper bound for the labels.\nfeatures_key (str):\n    Key for the features in the data files.\nlabel_key (str):\n    Key for the labels in the data files.\nout_framework (str):\n    Output framework.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.LoadTorchMLP","title":"LoadTorchMLP","text":"<pre><code>LoadTorchMLP(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models.</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.LoadTorchMLPInfer","title":"LoadTorchMLPInfer","text":"<pre><code>LoadTorchMLPInfer(model_file_path=None, network_config=None, input_dim=None)\n</code></pre> <p>Class to load TorchMLP models for inference. (This was originally useful directly for application in the HDDM toolbox).</p> Arguments <pre><code>model_file_path (str):\n    Path to the model file.\nnetwork_config (dict):\n    Network configuration.\ninput_dim (int):\n    Input dimension.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.LoadTorchMLPInfer.predict_on_batch","title":"predict_on_batch","text":"<pre><code>predict_on_batch(x=None)\n</code></pre> <p>Intended as function that computes trial wise log-likelihoods from a matrix input. To be used primarily through the HDDM toolbox.</p> Arguments <pre><code>x (numpy.ndarray(dtype=numpy.float32)):\n    Matrix which will be passed through the network.\n    LANs expect the matrix columns to follow a specific order.\n    When used in HDDM, x will be passed as follows.\n    The first few columns are trial wise model parameters\n    (order specified in the model_config file under the 'params' key).\n    The last two columns are filled with trial wise\n    reaction times and choices.\n    When not used via HDDM, no such restriction applies.\n</code></pre> Output <pre><code>numpy.ndarray(dtype = numpy.float32):\n    Output of the network. When called through HDDM,\n    this is expected as trial-wise log likelihoods\n    of a given generative model.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.ModelTrainerTorchMLP","title":"ModelTrainerTorchMLP","text":"<pre><code>ModelTrainerTorchMLP(train_config=None, model=None, train_dl=None, valid_dl=None, allow_abs_path_folder_generation=False, pin_memory=True, seed=None)\n</code></pre> <pre><code>train_config (dict):\n    Training configuration.\nmodel (TorchMLP):\n    TorchMLP model.\ntrain_dl (DatasetTorch):\n    Training dataloader.\nvalid_dl (DatasetTorch):\n    Validation dataloader.\nallow_abs_path_folder_generation (bool):\n    Whether to allow absolute path folder generation.\npin_memory (bool):\n    Whether to pin memory (dataloader). Can affect speed.\nseed (int):\n    Random seed.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.ModelTrainerTorchMLP.train_and_evaluate","title":"train_and_evaluate","text":"<pre><code>train_and_evaluate(output_folder='data/', output_file_id='fileid', run_id='runid', wandb_on=True, wandb_project_id='projectid', save_history=True, save_model=True, save_config=True, save_onnx=True, save_all=True, save_data_details=True, verbose=1)\n</code></pre> <p>Train and evaluate the model.</p> Arguments <pre><code>output_folder (str):\n    Output folder.\noutput_file_id (str):\n    Output file ID.\nrun_id (str):\n    Run ID.\nwandb_on (bool):\n    Whether to use wandb.\nwandb_project_id (str):\n    Wandb project ID.\nsave_history (bool):\n    Whether to save the training history.\nsave_model (bool):\n    Whether to save the model.\nsave_config (bool):\n    Whether to save the training configuration.\nsave_onnx (bool):\n    Whether to save the model to ONNX format.\nsave_all (bool):\n    Whether to save all.\nsave_data_details (bool):\n    Whether to save the data details.\nverbose (int):\n    Verbosity level.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.TorchMLP","title":"TorchMLP","text":"<pre><code>TorchMLP(network_config=None, input_shape=10, network_type=None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>TorchMLP class.</p> Arguments <pre><code>network_config (dict):\n    Network configuration.\ninput_shape (int):\n    Input shape.\nnetwork_type (str):\n    Network type.\n</code></pre>"},{"location":"api/trainers/#lanfactory.trainers.torch_mlp.TorchMLP.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass through network.</p> Arguments <pre><code>x (torch.Tensor):\n    Input tensor.\n</code></pre> <p>Returns:</p> <ul> <li> <code>    torch.Tensor:</code>           \u2013            <p>Output tensor.</p> </li> </ul>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#lanfactory.utils.save_configs","title":"lanfactory.utils.save_configs","text":"<pre><code>save_configs(model_id=None, save_folder=None, network_config=None, train_config=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to save the network and training configurations to a folder.</p> Arguments <pre><code>model_id (str):\n    The id of the model.\nsave_folder (str):\n    The folder to save the configurations to.\nnetwork_config (dict):\n    The network configuration dictionary.\ntrain_config (dict):\n    The training configuration dictionary.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/utils/#lanfactory.utils.try_gen_folder","title":"lanfactory.utils.try_gen_folder","text":"<pre><code>try_gen_folder(folder=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to generate a folder from a string. If the folder already exists, it will not be generated.</p> Arguments <pre><code>folder (str):\n    The folder string to generate.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/utils/#lanfactory.utils.util_funs","title":"lanfactory.utils.util_funs","text":""},{"location":"api/utils/#lanfactory.utils.util_funs.save_configs","title":"save_configs","text":"<pre><code>save_configs(model_id=None, save_folder=None, network_config=None, train_config=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to save the network and training configurations to a folder.</p> Arguments <pre><code>model_id (str):\n    The id of the model.\nsave_folder (str):\n    The folder to save the configurations to.\nnetwork_config (dict):\n    The network configuration dictionary.\ntrain_config (dict):\n    The training configuration dictionary.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"api/utils/#lanfactory.utils.util_funs.try_gen_folder","title":"try_gen_folder","text":"<pre><code>try_gen_folder(folder=None, allow_abs_path_folder_generation=True)\n</code></pre> <p>Function to generate a folder from a string. If the folder already exists, it will not be generated.</p> Arguments <pre><code>folder (str):\n    The folder string to generate.\nallow_abs_path_folder_generation (bool):\n    If True, the folder string is treated as an absolute path.\n    If False, the folder string is treated as a relative path.\n</code></pre>"},{"location":"basic_tutorial/basic_tutorial/","title":"Installation","text":"<p>The <code>LANfactory</code> package is a light-weight convenience package for training <code>likelihood approximation networks</code> (LANs) in torch (or keras), starting from supplied training data.</p> <p>LANs, although more general in potential scope of applications, were conceived in the context of sequential sampling modeling to account for cognitive processes giving rise to choice and reaction time data in n-alternative forced choice experiments commonly encountered in the cognitive sciences.</p> <p>In this quick tutorial we will use the <code>ssms</code> package to generate our training data using such a sequential sampling model (SSM). The use of of the <code>LANfactory</code> package is in no way bound to utilize this <code>ssms</code> package.</p> In\u00a0[1]: Copied! <pre># Load necessary packages\nimport ssms\nimport lanfactory\nimport os\nimport numpy as np\nfrom copy import deepcopy\nimport torch\n</pre> # Load necessary packages import ssms import lanfactory import os import numpy as np from copy import deepcopy import torch <pre>wandb not available\nwandb not available\n</pre> In\u00a0[2]: Copied! <pre># MAKE CONFIGS\nmodel = \"angle\"\n# Initialize the generator config (for MLP LANs)\ngenerator_config = deepcopy(ssms.config.data_generator_config[\"lan\"])\n# Specify generative model (one from the list of included models mentioned above)\ngenerator_config[\"model\"] = model\n# Specify number of parameter sets to simulate\ngenerator_config[\"n_parameter_sets\"] = 100\n# Specify how many samples a simulation run should entail\ngenerator_config[\"n_samples\"] = 1000\n# Specify folder in which to save generated data\ngenerator_config[\"output_folder\"] = \"data/lan_mlp/\" + model + \"/\"\n\n# Make model config dict\nmodel_config = ssms.config.model_config[model]\n</pre> # MAKE CONFIGS model = \"angle\" # Initialize the generator config (for MLP LANs) generator_config = deepcopy(ssms.config.data_generator_config[\"lan\"]) # Specify generative model (one from the list of included models mentioned above) generator_config[\"model\"] = model # Specify number of parameter sets to simulate generator_config[\"n_parameter_sets\"] = 100 # Specify how many samples a simulation run should entail generator_config[\"n_samples\"] = 1000 # Specify folder in which to save generated data generator_config[\"output_folder\"] = \"data/lan_mlp/\" + model + \"/\"  # Make model config dict model_config = ssms.config.model_config[model] In\u00a0[3]: Copied! <pre># MAKE DATA\nmy_dataset_generator = ssms.dataset_generators.lan_mlp.data_generator(\n    generator_config=generator_config, model_config=model_config\n)\n\ntraining_data = my_dataset_generator.generate_data_training_uniform(save=True)\n</pre> # MAKE DATA my_dataset_generator = ssms.dataset_generators.lan_mlp.data_generator(     generator_config=generator_config, model_config=model_config )  training_data = my_dataset_generator.generate_data_training_uniform(save=True) <pre>n_cpus used:  12\nchecking:  data/lan_mlp/angle/\nsimulation round: 1  of 10\nsimulation round: 2  of 10\nsimulation round: 3  of 10\nsimulation round: 4  of 10\nsimulation round: 5  of 10\nsimulation round: 6  of 10\nsimulation round: 7  of 10\nsimulation round: 8  of 10\nsimulation round: 9  of 10\nsimulation round: 10  of 10\nWriting to file:  data/lan_mlp/angle//training_data_6c3d3918ae0611efaf8d6ae25f443f63.pickle\n</pre> In\u00a0[4]: Copied! <pre># training_data\n</pre> # training_data In\u00a0[5]: Copied! <pre># MAKE DATALOADERS\n\n# List of datafiles (here only one)\nfolder_ = \"data/lan_mlp/\" + model + \"/\"  # + \"/training_data_0_nbins_0_n_1000/\"\nfile_list_ = [folder_ + file_ for file_ in os.listdir(folder_)]\n\n# Training dataset\ntorch_training_dataset = lanfactory.trainers.DatasetTorch(\n    file_ids=file_list_,\n    batch_size=128,\n    features_key=\"lan_data\",\n    label_key=\"lan_labels\",\n)\n\ntorch_training_dataloader = torch.utils.data.DataLoader(\n    torch_training_dataset,\n    shuffle=True,\n    batch_size=None,\n    num_workers=1,\n    pin_memory=True,\n)\n\n# Validation dataset\ntorch_validation_dataset = lanfactory.trainers.DatasetTorch(\n    file_ids=file_list_,\n    batch_size=128,\n    features_key=\"lan_data\",\n    label_key=\"lan_labels\",\n)\n\ntorch_validation_dataloader = torch.utils.data.DataLoader(\n    torch_validation_dataset,\n    shuffle=True,\n    batch_size=None,\n    num_workers=1,\n    pin_memory=True,\n)\n</pre> # MAKE DATALOADERS  # List of datafiles (here only one) folder_ = \"data/lan_mlp/\" + model + \"/\"  # + \"/training_data_0_nbins_0_n_1000/\" file_list_ = [folder_ + file_ for file_ in os.listdir(folder_)]  # Training dataset torch_training_dataset = lanfactory.trainers.DatasetTorch(     file_ids=file_list_,     batch_size=128,     features_key=\"lan_data\",     label_key=\"lan_labels\", )  torch_training_dataloader = torch.utils.data.DataLoader(     torch_training_dataset,     shuffle=True,     batch_size=None,     num_workers=1,     pin_memory=True, )  # Validation dataset torch_validation_dataset = lanfactory.trainers.DatasetTorch(     file_ids=file_list_,     batch_size=128,     features_key=\"lan_data\",     label_key=\"lan_labels\", )  torch_validation_dataloader = torch.utils.data.DataLoader(     torch_validation_dataset,     shuffle=True,     batch_size=None,     num_workers=1,     pin_memory=True, ) <p>Now we define two configuration dictionariers,</p> <ol> <li>The <code>network_config</code> dictionary defines the architecture and properties of the network</li> <li>The <code>train_config</code> dictionary defines properties concerning training hyperparameters</li> </ol> <p>Two examples (which we take as provided by the package, but which you can adjust according to your needs) are provided below.</p> In\u00a0[6]: Copied! <pre># SPECIFY NETWORK CONFIGS AND TRAINING CONFIGS\n\nnetwork_config = lanfactory.config.network_configs.network_config_mlp\n\nprint(\"Network config: \")\nprint(network_config)\n\ntrain_config = lanfactory.config.network_configs.train_config_mlp\n\nprint(\"Train config: \")\nprint(train_config)\n</pre> # SPECIFY NETWORK CONFIGS AND TRAINING CONFIGS  network_config = lanfactory.config.network_configs.network_config_mlp  print(\"Network config: \") print(network_config)  train_config = lanfactory.config.network_configs.train_config_mlp  print(\"Train config: \") print(train_config) <pre>Network config: \n{'layer_sizes': [100, 100, 1], 'activations': ['tanh', 'tanh', 'linear'], 'train_output_type': 'logprob'}\nTrain config: \n{'cpu_batch_size': 128, 'gpu_batch_size': 256, 'n_epochs': 5, 'optimizer': 'adam', 'learning_rate': 0.002, 'lr_scheduler': 'reduce_on_plateau', 'lr_scheduler_params': {}, 'weight_decay': 0.0, 'loss': 'huber', 'save_history': True}\n</pre> <p>We can now load a network, and save the configuration files for convenience.</p> In\u00a0[7]: Copied! <pre># LOAD NETWORK\nnet = lanfactory.trainers.TorchMLP(\n    network_config=deepcopy(network_config),\n    input_shape=torch_training_dataset.input_dim,\n    save_folder=\"/data/torch_models/\",\n    generative_model_id=\"angle\",\n)\n\n# SAVE CONFIGS\nlanfactory.utils.save_configs(\n    model_id=\"angle\" + \"_torch_\",\n    save_folder=\"data/torch_models/angle/\",\n    network_config=network_config,\n    train_config=train_config,\n    allow_abs_path_folder_generation=True,\n)\n</pre> # LOAD NETWORK net = lanfactory.trainers.TorchMLP(     network_config=deepcopy(network_config),     input_shape=torch_training_dataset.input_dim,     save_folder=\"/data/torch_models/\",     generative_model_id=\"angle\", )  # SAVE CONFIGS lanfactory.utils.save_configs(     model_id=\"angle\" + \"_torch_\",     save_folder=\"data/torch_models/angle/\",     network_config=network_config,     train_config=train_config,     allow_abs_path_folder_generation=True, ) <pre>Setting network type to \"lan\" or \"cpn\" based on train_output_type. \nNote: This is only a default setting, and can be overwritten by the network_type argument.\ntanh\ntanh\nlinear\nFound folder:  data\nMoving on...\nFound folder:  data/torch_models\nMoving on...\nFound folder:  data/torch_models/angle\nMoving on...\nSaved network config\nSaved train config\n</pre> <p>To finally train the network we supply our network, the dataloaders and training config to the <code>ModelTrainerTorchMLP</code> class, from <code>lanfactory.trainers</code>.</p> In\u00a0[8]: Copied! <pre># TRAIN MODEL\nmodel_trainer = lanfactory.trainers.ModelTrainerTorchMLP(\n    model=net,\n    train_config=train_config,\n    train_dl=torch_training_dataloader,\n    valid_dl=torch_validation_dataloader,\n    allow_abs_path_folder_generation=False,\n    pin_memory=True,\n    seed=None,\n)\n\n# model_trainer.train_model(save_history=True, save_model=True, verbose=0)\nmodel_trainer.train_and_evaluate()\n# LOAD MODEL\n</pre> # TRAIN MODEL model_trainer = lanfactory.trainers.ModelTrainerTorchMLP(     model=net,     train_config=train_config,     train_dl=torch_training_dataloader,     valid_dl=torch_validation_dataloader,     allow_abs_path_folder_generation=False,     pin_memory=True,     seed=None, )  # model_trainer.train_model(save_history=True, save_model=True, verbose=0) model_trainer.train_and_evaluate() # LOAD MODEL <pre>Torch Device:  cpu\ntrain_config is passed as dictionary: \n{'cpu_batch_size': 128, 'gpu_batch_size': 256, 'n_epochs': 5, 'optimizer': 'adam', 'learning_rate': 0.002, 'lr_scheduler': 'reduce_on_plateau', 'lr_scheduler_params': {}, 'weight_decay': 0.0, 'loss': 'huber', 'save_history': True}\n{'cpu_batch_size': 128, 'gpu_batch_size': 256, 'n_epochs': 5, 'optimizer': 'adam', 'learning_rate': 0.002, 'lr_scheduler': 'reduce_on_plateau', 'lr_scheduler_params': {}, 'weight_decay': 0.0, 'loss': 'huber', 'save_history': True}\nFound folder:  data\nMoving on...\nname 'wandb' is not defined\nwandb not available, not storing results there\npassing 4\nname 'wandb' is not defined\n</pre> <pre>/Users/afengler/miniconda3/envs/lanfactory/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n</pre> <pre>wandb not available\nwandb not available\nepoch: 0 / 5, batch: 0 / 6248, batch_loss: 18.424171447753906\nepoch: 0 / 5, batch: 100 / 6248, batch_loss: 26.35689926147461\nepoch: 0 / 5, batch: 200 / 6248, batch_loss: 21.270301818847656\nepoch: 0 / 5, batch: 300 / 6248, batch_loss: 41.69943618774414\nepoch: 0 / 5, batch: 400 / 6248, batch_loss: 12.145707130432129\nepoch: 0 / 5, batch: 500 / 6248, batch_loss: 7.219738006591797\nepoch: 0 / 5, batch: 600 / 6248, batch_loss: 14.006623268127441\nepoch: 0 / 5, batch: 700 / 6248, batch_loss: 13.515326499938965\nepoch: 0 / 5, batch: 800 / 6248, batch_loss: 17.281633377075195\nepoch: 0 / 5, batch: 900 / 6248, batch_loss: 7.4594902992248535\nepoch: 0 / 5, batch: 1000 / 6248, batch_loss: 20.00450897216797\nepoch: 0 / 5, batch: 1100 / 6248, batch_loss: 7.115589141845703\nepoch: 0 / 5, batch: 1200 / 6248, batch_loss: 6.586312770843506\nepoch: 0 / 5, batch: 1300 / 6248, batch_loss: 8.359161376953125\nepoch: 0 / 5, batch: 1400 / 6248, batch_loss: 10.252017974853516\nepoch: 0 / 5, batch: 1500 / 6248, batch_loss: 7.556928634643555\nepoch: 0 / 5, batch: 1600 / 6248, batch_loss: 9.202360153198242\nepoch: 0 / 5, batch: 1700 / 6248, batch_loss: 14.47900390625\nepoch: 0 / 5, batch: 1800 / 6248, batch_loss: 15.528148651123047\nepoch: 0 / 5, batch: 1900 / 6248, batch_loss: 22.215539932250977\nepoch: 0 / 5, batch: 2000 / 6248, batch_loss: 28.170188903808594\nepoch: 0 / 5, batch: 2100 / 6248, batch_loss: 117.04457092285156\nepoch: 0 / 5, batch: 2200 / 6248, batch_loss: 9.00933837890625\nepoch: 0 / 5, batch: 2300 / 6248, batch_loss: 15.27622127532959\nepoch: 0 / 5, batch: 2400 / 6248, batch_loss: 28.07713508605957\nepoch: 0 / 5, batch: 2500 / 6248, batch_loss: 5.567930221557617\nepoch: 0 / 5, batch: 2600 / 6248, batch_loss: 5.655803203582764\nepoch: 0 / 5, batch: 2700 / 6248, batch_loss: 4.361502647399902\nepoch: 0 / 5, batch: 2800 / 6248, batch_loss: 16.26578140258789\nepoch: 0 / 5, batch: 2900 / 6248, batch_loss: 15.86043643951416\nepoch: 0 / 5, batch: 3000 / 6248, batch_loss: 18.201242446899414\nepoch: 0 / 5, batch: 3100 / 6248, batch_loss: 5.630181789398193\nepoch: 0 / 5, batch: 3200 / 6248, batch_loss: 5.413924694061279\nepoch: 0 / 5, batch: 3300 / 6248, batch_loss: 10.452692031860352\nepoch: 0 / 5, batch: 3400 / 6248, batch_loss: 5.651644229888916\nepoch: 0 / 5, batch: 3500 / 6248, batch_loss: 2.2360763549804688\nepoch: 0 / 5, batch: 3600 / 6248, batch_loss: 6.055055618286133\nepoch: 0 / 5, batch: 3700 / 6248, batch_loss: 8.54580307006836\nepoch: 0 / 5, batch: 3800 / 6248, batch_loss: 4.23875093460083\nepoch: 0 / 5, batch: 3900 / 6248, batch_loss: 7.544300556182861\nepoch: 0 / 5, batch: 4000 / 6248, batch_loss: 6.990339756011963\nepoch: 0 / 5, batch: 4100 / 6248, batch_loss: 5.147078990936279\nepoch: 0 / 5, batch: 4200 / 6248, batch_loss: 16.651050567626953\nepoch: 0 / 5, batch: 4300 / 6248, batch_loss: 6.522802352905273\nepoch: 0 / 5, batch: 4400 / 6248, batch_loss: 7.322534561157227\nepoch: 0 / 5, batch: 4500 / 6248, batch_loss: 3.5635647773742676\nepoch: 0 / 5, batch: 4600 / 6248, batch_loss: 19.539169311523438\nepoch: 0 / 5, batch: 4700 / 6248, batch_loss: 2.392941474914551\nepoch: 0 / 5, batch: 4800 / 6248, batch_loss: 6.605629920959473\nepoch: 0 / 5, batch: 4900 / 6248, batch_loss: 11.018367767333984\nepoch: 0 / 5, batch: 5000 / 6248, batch_loss: 9.82219409942627\nepoch: 0 / 5, batch: 5100 / 6248, batch_loss: 3.830484628677368\nepoch: 0 / 5, batch: 5200 / 6248, batch_loss: 1.8000483512878418\nepoch: 0 / 5, batch: 5300 / 6248, batch_loss: 16.260841369628906\nepoch: 0 / 5, batch: 5400 / 6248, batch_loss: 17.147472381591797\nepoch: 0 / 5, batch: 5500 / 6248, batch_loss: 10.085953712463379\nepoch: 0 / 5, batch: 5600 / 6248, batch_loss: 6.210273742675781\nepoch: 0 / 5, batch: 5700 / 6248, batch_loss: 5.421726703643799\nepoch: 0 / 5, batch: 5800 / 6248, batch_loss: 25.590669631958008\nepoch: 0 / 5, batch: 5900 / 6248, batch_loss: 2.8900420665740967\nepoch: 0 / 5, batch: 6000 / 6248, batch_loss: 3.811896562576294\nepoch: 0 / 5, batch: 6100 / 6248, batch_loss: 0.8129540681838989\nepoch: 0 / 5, batch: 6200 / 6248, batch_loss: 0.9393517971038818\nEpoch took 0 / 5,  took 26.938950777053833 seconds\nwandb not available\nwandb not available\nepoch 0 / 5, validation_loss: 12.69\npassing 5\nname 'wandb' is not defined\nwandb not available\nwandb not available\nepoch: 1 / 5, batch: 0 / 6248, batch_loss: 2.916593074798584\nepoch: 1 / 5, batch: 100 / 6248, batch_loss: 6.199081897735596\nepoch: 1 / 5, batch: 200 / 6248, batch_loss: 10.125048637390137\nepoch: 1 / 5, batch: 300 / 6248, batch_loss: 32.554256439208984\nepoch: 1 / 5, batch: 400 / 6248, batch_loss: 16.39636993408203\nepoch: 1 / 5, batch: 500 / 6248, batch_loss: 7.576246738433838\nepoch: 1 / 5, batch: 600 / 6248, batch_loss: 17.355613708496094\nepoch: 1 / 5, batch: 700 / 6248, batch_loss: 11.501689910888672\nepoch: 1 / 5, batch: 800 / 6248, batch_loss: 3.552396774291992\nepoch: 1 / 5, batch: 900 / 6248, batch_loss: 18.60572624206543\nepoch: 1 / 5, batch: 1000 / 6248, batch_loss: 22.339616775512695\nepoch: 1 / 5, batch: 1100 / 6248, batch_loss: 9.483509063720703\nepoch: 1 / 5, batch: 1200 / 6248, batch_loss: 10.513774871826172\nepoch: 1 / 5, batch: 1300 / 6248, batch_loss: 1.6283797025680542\nepoch: 1 / 5, batch: 1400 / 6248, batch_loss: 6.762075901031494\nepoch: 1 / 5, batch: 1500 / 6248, batch_loss: 21.324249267578125\nepoch: 1 / 5, batch: 1600 / 6248, batch_loss: 25.508773803710938\nepoch: 1 / 5, batch: 1700 / 6248, batch_loss: 1.1320406198501587\nepoch: 1 / 5, batch: 1800 / 6248, batch_loss: 2.2140390872955322\nepoch: 1 / 5, batch: 1900 / 6248, batch_loss: 18.65582275390625\nepoch: 1 / 5, batch: 2000 / 6248, batch_loss: 9.36327838897705\nepoch: 1 / 5, batch: 2100 / 6248, batch_loss: 10.27192211151123\nepoch: 1 / 5, batch: 2200 / 6248, batch_loss: 2.4746015071868896\nepoch: 1 / 5, batch: 2300 / 6248, batch_loss: 14.252975463867188\nepoch: 1 / 5, batch: 2400 / 6248, batch_loss: 22.501874923706055\nepoch: 1 / 5, batch: 2500 / 6248, batch_loss: 2.3512606620788574\nepoch: 1 / 5, batch: 2600 / 6248, batch_loss: 3.6053552627563477\nepoch: 1 / 5, batch: 2700 / 6248, batch_loss: 16.11327362060547\nepoch: 1 / 5, batch: 2800 / 6248, batch_loss: 4.100347995758057\nepoch: 1 / 5, batch: 2900 / 6248, batch_loss: 1.8121297359466553\nepoch: 1 / 5, batch: 3000 / 6248, batch_loss: 6.521814346313477\nepoch: 1 / 5, batch: 3100 / 6248, batch_loss: 5.876171588897705\nepoch: 1 / 5, batch: 3200 / 6248, batch_loss: 4.293362617492676\nepoch: 1 / 5, batch: 3300 / 6248, batch_loss: 5.608534336090088\nepoch: 1 / 5, batch: 3400 / 6248, batch_loss: 2.534605026245117\nepoch: 1 / 5, batch: 3500 / 6248, batch_loss: 3.059785842895508\nepoch: 1 / 5, batch: 3600 / 6248, batch_loss: 34.93362045288086\nepoch: 1 / 5, batch: 3700 / 6248, batch_loss: 18.400196075439453\nepoch: 1 / 5, batch: 3800 / 6248, batch_loss: 12.421780586242676\nepoch: 1 / 5, batch: 3900 / 6248, batch_loss: 9.627634048461914\nepoch: 1 / 5, batch: 4000 / 6248, batch_loss: 9.883007049560547\nepoch: 1 / 5, batch: 4100 / 6248, batch_loss: 24.601343154907227\nepoch: 1 / 5, batch: 4200 / 6248, batch_loss: 3.733724594116211\nepoch: 1 / 5, batch: 4300 / 6248, batch_loss: 25.589256286621094\nepoch: 1 / 5, batch: 4400 / 6248, batch_loss: 8.407793998718262\nepoch: 1 / 5, batch: 4500 / 6248, batch_loss: 1.862131953239441\nepoch: 1 / 5, batch: 4600 / 6248, batch_loss: 7.546970367431641\nepoch: 1 / 5, batch: 4700 / 6248, batch_loss: 15.335176467895508\nepoch: 1 / 5, batch: 4800 / 6248, batch_loss: 1.721323847770691\nepoch: 1 / 5, batch: 4900 / 6248, batch_loss: 5.1945295333862305\nepoch: 1 / 5, batch: 5000 / 6248, batch_loss: 2.6280040740966797\nepoch: 1 / 5, batch: 5100 / 6248, batch_loss: 1.9248709678649902\nepoch: 1 / 5, batch: 5200 / 6248, batch_loss: 1.7881675958633423\nepoch: 1 / 5, batch: 5300 / 6248, batch_loss: 7.1162824630737305\nepoch: 1 / 5, batch: 5400 / 6248, batch_loss: 21.415637969970703\nepoch: 1 / 5, batch: 5500 / 6248, batch_loss: 3.9836742877960205\nepoch: 1 / 5, batch: 5600 / 6248, batch_loss: 1.6126636266708374\nepoch: 1 / 5, batch: 5700 / 6248, batch_loss: 15.508781433105469\nepoch: 1 / 5, batch: 5800 / 6248, batch_loss: 4.549023628234863\nepoch: 1 / 5, batch: 5900 / 6248, batch_loss: 3.9345669746398926\nepoch: 1 / 5, batch: 6000 / 6248, batch_loss: 5.369487762451172\nepoch: 1 / 5, batch: 6100 / 6248, batch_loss: 9.147500991821289\nepoch: 1 / 5, batch: 6200 / 6248, batch_loss: 2.971890926361084\nEpoch took 1 / 5,  took 31.633978128433228 seconds\nwandb not available\nwandb not available\nepoch 1 / 5, validation_loss: 9.103\npassing 5\nname 'wandb' is not defined\nwandb not available\nwandb not available\nepoch: 2 / 5, batch: 0 / 6248, batch_loss: 18.177335739135742\nepoch: 2 / 5, batch: 100 / 6248, batch_loss: 7.160423755645752\nepoch: 2 / 5, batch: 200 / 6248, batch_loss: 1.0833526849746704\nepoch: 2 / 5, batch: 300 / 6248, batch_loss: 0.7689553499221802\nepoch: 2 / 5, batch: 400 / 6248, batch_loss: 7.863158226013184\nepoch: 2 / 5, batch: 500 / 6248, batch_loss: 31.456398010253906\nepoch: 2 / 5, batch: 600 / 6248, batch_loss: 1.6489198207855225\nepoch: 2 / 5, batch: 700 / 6248, batch_loss: 7.911120414733887\nepoch: 2 / 5, batch: 800 / 6248, batch_loss: 2.726231336593628\nepoch: 2 / 5, batch: 900 / 6248, batch_loss: 2.043830156326294\nepoch: 2 / 5, batch: 1000 / 6248, batch_loss: 17.249248504638672\nepoch: 2 / 5, batch: 1100 / 6248, batch_loss: 1.9473541975021362\nepoch: 2 / 5, batch: 1200 / 6248, batch_loss: 4.596618175506592\nepoch: 2 / 5, batch: 1300 / 6248, batch_loss: 7.390196323394775\nepoch: 2 / 5, batch: 1400 / 6248, batch_loss: 4.65122652053833\nepoch: 2 / 5, batch: 1500 / 6248, batch_loss: 1.6893219947814941\nepoch: 2 / 5, batch: 1600 / 6248, batch_loss: 1.5878593921661377\nepoch: 2 / 5, batch: 1700 / 6248, batch_loss: 12.68493366241455\nepoch: 2 / 5, batch: 1800 / 6248, batch_loss: 1.863877534866333\nepoch: 2 / 5, batch: 1900 / 6248, batch_loss: 8.68303108215332\nepoch: 2 / 5, batch: 2000 / 6248, batch_loss: 1.3251135349273682\nepoch: 2 / 5, batch: 2100 / 6248, batch_loss: 9.0327730178833\nepoch: 2 / 5, batch: 2200 / 6248, batch_loss: 1.561706304550171\nepoch: 2 / 5, batch: 2300 / 6248, batch_loss: 3.551076650619507\nepoch: 2 / 5, batch: 2400 / 6248, batch_loss: 1.4739888906478882\nepoch: 2 / 5, batch: 2500 / 6248, batch_loss: 3.4899544715881348\nepoch: 2 / 5, batch: 2600 / 6248, batch_loss: 2.405264377593994\nepoch: 2 / 5, batch: 2700 / 6248, batch_loss: 9.408719062805176\nepoch: 2 / 5, batch: 2800 / 6248, batch_loss: 3.167339563369751\nepoch: 2 / 5, batch: 2900 / 6248, batch_loss: 3.569464683532715\nepoch: 2 / 5, batch: 3000 / 6248, batch_loss: 1.433802843093872\nepoch: 2 / 5, batch: 3100 / 6248, batch_loss: 10.608311653137207\nepoch: 2 / 5, batch: 3200 / 6248, batch_loss: 10.512945175170898\nepoch: 2 / 5, batch: 3300 / 6248, batch_loss: 28.54212188720703\nepoch: 2 / 5, batch: 3400 / 6248, batch_loss: 1.9291532039642334\nepoch: 2 / 5, batch: 3500 / 6248, batch_loss: 0.6310811638832092\nepoch: 2 / 5, batch: 3600 / 6248, batch_loss: 1.8336838483810425\nepoch: 2 / 5, batch: 3700 / 6248, batch_loss: 5.743785381317139\nepoch: 2 / 5, batch: 3800 / 6248, batch_loss: 6.902609348297119\nepoch: 2 / 5, batch: 3900 / 6248, batch_loss: 1.1317849159240723\nepoch: 2 / 5, batch: 4000 / 6248, batch_loss: 0.6954737305641174\nepoch: 2 / 5, batch: 4100 / 6248, batch_loss: 2.868868350982666\nepoch: 2 / 5, batch: 4200 / 6248, batch_loss: 3.122647285461426\nepoch: 2 / 5, batch: 4300 / 6248, batch_loss: 3.0988526344299316\nepoch: 2 / 5, batch: 4400 / 6248, batch_loss: 6.4876532554626465\nepoch: 2 / 5, batch: 4500 / 6248, batch_loss: 0.3020702600479126\nepoch: 2 / 5, batch: 4600 / 6248, batch_loss: 1.7590858936309814\nepoch: 2 / 5, batch: 4700 / 6248, batch_loss: 2.8763742446899414\nepoch: 2 / 5, batch: 4800 / 6248, batch_loss: 2.724623680114746\nepoch: 2 / 5, batch: 4900 / 6248, batch_loss: 5.63917350769043\nepoch: 2 / 5, batch: 5000 / 6248, batch_loss: 20.56009864807129\nepoch: 2 / 5, batch: 5100 / 6248, batch_loss: 13.702452659606934\nepoch: 2 / 5, batch: 5200 / 6248, batch_loss: 2.0691349506378174\nepoch: 2 / 5, batch: 5300 / 6248, batch_loss: 10.479930877685547\nepoch: 2 / 5, batch: 5400 / 6248, batch_loss: 0.8936798572540283\nepoch: 2 / 5, batch: 5500 / 6248, batch_loss: 6.050797939300537\nepoch: 2 / 5, batch: 5600 / 6248, batch_loss: 1.5894570350646973\nepoch: 2 / 5, batch: 5700 / 6248, batch_loss: 4.0408525466918945\nepoch: 2 / 5, batch: 5800 / 6248, batch_loss: 8.634786605834961\nepoch: 2 / 5, batch: 5900 / 6248, batch_loss: 1.6445071697235107\nepoch: 2 / 5, batch: 6000 / 6248, batch_loss: 3.3243021965026855\nepoch: 2 / 5, batch: 6100 / 6248, batch_loss: 1.1728134155273438\nepoch: 2 / 5, batch: 6200 / 6248, batch_loss: 4.719942569732666\nEpoch took 2 / 5,  took 28.473190784454346 seconds\nwandb not available\nwandb not available\nepoch 2 / 5, validation_loss: 7.6\npassing 5\nname 'wandb' is not defined\nwandb not available\nwandb not available\nepoch: 3 / 5, batch: 0 / 6248, batch_loss: 11.20986270904541\nepoch: 3 / 5, batch: 100 / 6248, batch_loss: 4.879384994506836\nepoch: 3 / 5, batch: 200 / 6248, batch_loss: 10.518756866455078\nepoch: 3 / 5, batch: 300 / 6248, batch_loss: 1.7004778385162354\nepoch: 3 / 5, batch: 400 / 6248, batch_loss: 10.379594802856445\nepoch: 3 / 5, batch: 500 / 6248, batch_loss: 3.0820789337158203\nepoch: 3 / 5, batch: 600 / 6248, batch_loss: 6.677118301391602\nepoch: 3 / 5, batch: 700 / 6248, batch_loss: 1.2418594360351562\nepoch: 3 / 5, batch: 800 / 6248, batch_loss: 2.9810616970062256\nepoch: 3 / 5, batch: 900 / 6248, batch_loss: 2.4596235752105713\nepoch: 3 / 5, batch: 1000 / 6248, batch_loss: 12.875043869018555\nepoch: 3 / 5, batch: 1100 / 6248, batch_loss: 1.7667275667190552\nepoch: 3 / 5, batch: 1200 / 6248, batch_loss: 2.129171848297119\nepoch: 3 / 5, batch: 1300 / 6248, batch_loss: 3.8405673503875732\nepoch: 3 / 5, batch: 1400 / 6248, batch_loss: 6.10286283493042\nepoch: 3 / 5, batch: 1500 / 6248, batch_loss: 2.039433240890503\nepoch: 3 / 5, batch: 1600 / 6248, batch_loss: 0.7429957985877991\nepoch: 3 / 5, batch: 1700 / 6248, batch_loss: 3.034165382385254\nepoch: 3 / 5, batch: 1800 / 6248, batch_loss: 8.705617904663086\nepoch: 3 / 5, batch: 1900 / 6248, batch_loss: 3.8629262447357178\nepoch: 3 / 5, batch: 2000 / 6248, batch_loss: 22.68282127380371\nepoch: 3 / 5, batch: 2100 / 6248, batch_loss: 0.9397985339164734\nepoch: 3 / 5, batch: 2200 / 6248, batch_loss: 1.8767139911651611\nepoch: 3 / 5, batch: 2300 / 6248, batch_loss: 3.945666551589966\nepoch: 3 / 5, batch: 2400 / 6248, batch_loss: 5.117251873016357\nepoch: 3 / 5, batch: 2500 / 6248, batch_loss: 3.562479019165039\nepoch: 3 / 5, batch: 2600 / 6248, batch_loss: 9.357687950134277\nepoch: 3 / 5, batch: 2700 / 6248, batch_loss: 4.645717144012451\nepoch: 3 / 5, batch: 2800 / 6248, batch_loss: 0.5134652256965637\nepoch: 3 / 5, batch: 2900 / 6248, batch_loss: 3.4245641231536865\nepoch: 3 / 5, batch: 3000 / 6248, batch_loss: 1.4013506174087524\nepoch: 3 / 5, batch: 3100 / 6248, batch_loss: 3.5509908199310303\nepoch: 3 / 5, batch: 3200 / 6248, batch_loss: 11.406017303466797\nepoch: 3 / 5, batch: 3300 / 6248, batch_loss: 0.497477650642395\nepoch: 3 / 5, batch: 3400 / 6248, batch_loss: 19.900798797607422\nepoch: 3 / 5, batch: 3500 / 6248, batch_loss: 15.575096130371094\nepoch: 3 / 5, batch: 3600 / 6248, batch_loss: 4.218257904052734\nepoch: 3 / 5, batch: 3700 / 6248, batch_loss: 6.017703533172607\nepoch: 3 / 5, batch: 3800 / 6248, batch_loss: 4.993248462677002\nepoch: 3 / 5, batch: 3900 / 6248, batch_loss: 3.6399292945861816\nepoch: 3 / 5, batch: 4000 / 6248, batch_loss: 9.613175392150879\nepoch: 3 / 5, batch: 4100 / 6248, batch_loss: 4.65733003616333\nepoch: 3 / 5, batch: 4200 / 6248, batch_loss: 12.048975944519043\nepoch: 3 / 5, batch: 4300 / 6248, batch_loss: 3.8053112030029297\nepoch: 3 / 5, batch: 4400 / 6248, batch_loss: 7.079024791717529\nepoch: 3 / 5, batch: 4500 / 6248, batch_loss: 1.0926227569580078\nepoch: 3 / 5, batch: 4600 / 6248, batch_loss: 0.8070131540298462\nepoch: 3 / 5, batch: 4700 / 6248, batch_loss: 0.5585042238235474\nepoch: 3 / 5, batch: 4800 / 6248, batch_loss: 0.5513670444488525\nepoch: 3 / 5, batch: 4900 / 6248, batch_loss: 0.5275300145149231\nepoch: 3 / 5, batch: 5000 / 6248, batch_loss: 6.935880661010742\nepoch: 3 / 5, batch: 5100 / 6248, batch_loss: 5.937623023986816\nepoch: 3 / 5, batch: 5200 / 6248, batch_loss: 2.511467695236206\nepoch: 3 / 5, batch: 5300 / 6248, batch_loss: 5.006554126739502\nepoch: 3 / 5, batch: 5400 / 6248, batch_loss: 36.16102600097656\nepoch: 3 / 5, batch: 5500 / 6248, batch_loss: 0.6847551465034485\nepoch: 3 / 5, batch: 5600 / 6248, batch_loss: 3.9064245223999023\nepoch: 3 / 5, batch: 5700 / 6248, batch_loss: 0.49317577481269836\nepoch: 3 / 5, batch: 5800 / 6248, batch_loss: 1.3496861457824707\nepoch: 3 / 5, batch: 5900 / 6248, batch_loss: 1.7833096981048584\nepoch: 3 / 5, batch: 6000 / 6248, batch_loss: 4.133869647979736\nepoch: 3 / 5, batch: 6100 / 6248, batch_loss: 14.814723014831543\nepoch: 3 / 5, batch: 6200 / 6248, batch_loss: 16.379371643066406\nEpoch took 3 / 5,  took 29.750263214111328 seconds\nwandb not available\nwandb not available\nepoch 3 / 5, validation_loss: 7.5\npassing 5\nname 'wandb' is not defined\nwandb not available\nwandb not available\nepoch: 4 / 5, batch: 0 / 6248, batch_loss: 5.8787946701049805\nepoch: 4 / 5, batch: 100 / 6248, batch_loss: 1.6356234550476074\nepoch: 4 / 5, batch: 200 / 6248, batch_loss: 4.545633792877197\nepoch: 4 / 5, batch: 300 / 6248, batch_loss: 0.6724920272827148\nepoch: 4 / 5, batch: 400 / 6248, batch_loss: 1.5256768465042114\nepoch: 4 / 5, batch: 500 / 6248, batch_loss: 13.512426376342773\nepoch: 4 / 5, batch: 600 / 6248, batch_loss: 12.820659637451172\nepoch: 4 / 5, batch: 700 / 6248, batch_loss: 1.34501051902771\nepoch: 4 / 5, batch: 800 / 6248, batch_loss: 8.646514892578125\nepoch: 4 / 5, batch: 900 / 6248, batch_loss: 3.8529038429260254\nepoch: 4 / 5, batch: 1000 / 6248, batch_loss: 8.1100492477417\nepoch: 4 / 5, batch: 1100 / 6248, batch_loss: 1.5255681276321411\nepoch: 4 / 5, batch: 1200 / 6248, batch_loss: 7.932858467102051\nepoch: 4 / 5, batch: 1300 / 6248, batch_loss: 98.44223022460938\nepoch: 4 / 5, batch: 1400 / 6248, batch_loss: 6.510103225708008\nepoch: 4 / 5, batch: 1500 / 6248, batch_loss: 1.4880013465881348\nepoch: 4 / 5, batch: 1600 / 6248, batch_loss: 1.0249238014221191\nepoch: 4 / 5, batch: 1700 / 6248, batch_loss: 1.373228669166565\nepoch: 4 / 5, batch: 1800 / 6248, batch_loss: 6.032616138458252\nepoch: 4 / 5, batch: 1900 / 6248, batch_loss: 3.3218276500701904\nepoch: 4 / 5, batch: 2000 / 6248, batch_loss: 1.3221908807754517\nepoch: 4 / 5, batch: 2100 / 6248, batch_loss: 6.059354782104492\nepoch: 4 / 5, batch: 2200 / 6248, batch_loss: 8.933648109436035\nepoch: 4 / 5, batch: 2300 / 6248, batch_loss: 2.414980173110962\nepoch: 4 / 5, batch: 2400 / 6248, batch_loss: 1.2065949440002441\nepoch: 4 / 5, batch: 2500 / 6248, batch_loss: 2.1951138973236084\nepoch: 4 / 5, batch: 2600 / 6248, batch_loss: 1.74544095993042\nepoch: 4 / 5, batch: 2700 / 6248, batch_loss: 2.0079307556152344\nepoch: 4 / 5, batch: 2800 / 6248, batch_loss: 1.1665210723876953\nepoch: 4 / 5, batch: 2900 / 6248, batch_loss: 0.790613055229187\nepoch: 4 / 5, batch: 3000 / 6248, batch_loss: 3.746497631072998\nepoch: 4 / 5, batch: 3100 / 6248, batch_loss: 6.304383754730225\nepoch: 4 / 5, batch: 3200 / 6248, batch_loss: 3.4822640419006348\nepoch: 4 / 5, batch: 3300 / 6248, batch_loss: 4.5432844161987305\nepoch: 4 / 5, batch: 3400 / 6248, batch_loss: 3.0952796936035156\nepoch: 4 / 5, batch: 3500 / 6248, batch_loss: 2.022463798522949\nepoch: 4 / 5, batch: 3600 / 6248, batch_loss: 0.32596150040626526\nepoch: 4 / 5, batch: 3700 / 6248, batch_loss: 5.023234844207764\nepoch: 4 / 5, batch: 3800 / 6248, batch_loss: 25.301870346069336\nepoch: 4 / 5, batch: 3900 / 6248, batch_loss: 8.250364303588867\nepoch: 4 / 5, batch: 4000 / 6248, batch_loss: 18.745418548583984\nepoch: 4 / 5, batch: 4100 / 6248, batch_loss: 1.3826156854629517\nepoch: 4 / 5, batch: 4200 / 6248, batch_loss: 3.672250270843506\nepoch: 4 / 5, batch: 4300 / 6248, batch_loss: 13.193106651306152\nepoch: 4 / 5, batch: 4400 / 6248, batch_loss: 0.753337562084198\nepoch: 4 / 5, batch: 4500 / 6248, batch_loss: 1.8329994678497314\nepoch: 4 / 5, batch: 4600 / 6248, batch_loss: 7.841012954711914\nepoch: 4 / 5, batch: 4700 / 6248, batch_loss: 2.2451441287994385\nepoch: 4 / 5, batch: 4800 / 6248, batch_loss: 5.123344421386719\nepoch: 4 / 5, batch: 4900 / 6248, batch_loss: 15.224817276000977\nepoch: 4 / 5, batch: 5000 / 6248, batch_loss: 0.6639282703399658\nepoch: 4 / 5, batch: 5100 / 6248, batch_loss: 2.339409828186035\nepoch: 4 / 5, batch: 5200 / 6248, batch_loss: 0.4993884563446045\nepoch: 4 / 5, batch: 5300 / 6248, batch_loss: 0.6163206100463867\nepoch: 4 / 5, batch: 5400 / 6248, batch_loss: 1.4182488918304443\nepoch: 4 / 5, batch: 5500 / 6248, batch_loss: 6.47085428237915\nepoch: 4 / 5, batch: 5600 / 6248, batch_loss: 12.12121295928955\nepoch: 4 / 5, batch: 5700 / 6248, batch_loss: 5.5967607498168945\nepoch: 4 / 5, batch: 5800 / 6248, batch_loss: 25.024938583374023\nepoch: 4 / 5, batch: 5900 / 6248, batch_loss: 1.4893072843551636\nepoch: 4 / 5, batch: 6000 / 6248, batch_loss: 1.1123461723327637\nepoch: 4 / 5, batch: 6100 / 6248, batch_loss: 0.9011953473091125\nepoch: 4 / 5, batch: 6200 / 6248, batch_loss: 2.996565580368042\nEpoch took 4 / 5,  took 26.869418144226074 seconds\nwandb not available\nwandb not available\nepoch 4 / 5, validation_loss: 6.278\npassing 5\nname 'wandb' is not defined\nSaving training history\nSaving training history to: data//fileid_lan_runid_torch_training_history.csv\nSaving model state dict\nSaving model parameters to: data//fileid_lan_runid_train_state_dict_torch.pt\nSaving training config to: data//fileid_lan_runid_train_config.pickle\nSaving training data details to: data//fileid_lan_runid_data_details.pickle\nSaving model to ONNX format\nSaving model to ONNX format to: data//fileid_lan_runid_torch_model.onnx\npassing 6\nname 'wandb' is not defined\nTraining finished successfully...\n</pre> <p>The <code>LANfactory</code> provides some convenience functions to use networks for inference after training. We can load a model using the <code>LoadTorchMLPInfer</code> class, which then allows us to run fast inference via either a direct call, which expects a <code>torch.tensor</code> as input, or the <code>predict_on_batch</code> method, which expects a <code>numpy.array</code> of <code>dtype</code>, <code>np.float32</code>.</p> In\u00a0[9]: Copied! <pre>network_path_list = os.listdir(\"data/torch_models/angle/\")\nnetwork_file_path = [\n    \"data/torch_models/angle/\" + file_\n    for file_ in network_path_list\n    if \"state_dict\" in file_\n][0]\n\nnetwork = lanfactory.trainers.LoadTorchMLPInfer(\n    model_file_path=network_file_path,\n    network_config=network_config,\n    input_dim=torch_training_dataset.input_dim,\n)\n</pre> network_path_list = os.listdir(\"data/torch_models/angle/\") network_file_path = [     \"data/torch_models/angle/\" + file_     for file_ in network_path_list     if \"state_dict\" in file_ ][0]  network = lanfactory.trainers.LoadTorchMLPInfer(     model_file_path=network_file_path,     network_config=network_config,     input_dim=torch_training_dataset.input_dim, ) <pre>Setting network type to \"lan\" or \"cpn\" based on train_output_type. \nNote: This is only a default setting, and can be overwritten by the network_type argument.\ntanh\ntanh\nlinear\n</pre> <pre>/Users/afengler/Library/CloudStorage/OneDrive-Personal/project_lanfactory/LANfactory/lanfactory/trainers/torch_mlp.py:704: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.net.load_state_dict(torch.load(self.model_file_path))\n</pre> In\u00a0[10]: Copied! <pre># Two ways to call the network\n\n# Direct call --&gt; need tensor input\ndirect_out = network(\n    torch.from_numpy(np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype=np.float32))\n)\nprint(\"direct call out: \", direct_out)\n\n# predict_on_batch method\npredict_on_batch_out = network.predict_on_batch(\n    np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype=np.float32)\n)\nprint(\"predict_on_batch out: \", predict_on_batch_out)\n</pre> # Two ways to call the network  # Direct call --&gt; need tensor input direct_out = network(     torch.from_numpy(np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype=np.float32)) ) print(\"direct call out: \", direct_out)  # predict_on_batch method predict_on_batch_out = network.predict_on_batch(     np.array([1, 1.5, 0.5, 1.0, 0.1, 0.65, 1], dtype=np.float32) ) print(\"predict_on_batch out: \", predict_on_batch_out) <pre>direct call out:  tensor([-66.0045])\npredict_on_batch out:  [-66.00449]\n</pre> In\u00a0[55]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom ssms.basic_simulators.simulator import simulator\n\ndata = pd.DataFrame(\n    np.zeros((2000, 7), dtype=np.float32),\n    columns=[\"v\", \"a\", \"z\", \"t\", \"theta\", \"rt\", \"choice\"],\n)\ndata[\"v\"] = 0.5\ndata[\"a\"] = 0.75\ndata[\"z\"] = 0.5\ndata[\"t\"] = 0.2\ndata[\"theta\"] = 0.1\ndata.loc[np.arange(0, 1000), \"rt\"] = np.linspace(5, 0, 1000).astype(np.float32)\ndata.loc[np.arange(1000, 2000), \"rt\"] = np.linspace(0, 5, 1000).astype(np.float32)\ndata.loc[np.arange(0, 1000), \"choice\"] = -1\ndata.loc[np.arange(1000, 2000), \"choice\"] = 1\n\n# Network predictions\npredict_on_batch_out = network.predict_on_batch(data.values.astype(np.float32))\n\n# Simulations\nsim_out = simulator(model=\"angle\", theta=data.values[0, :-2], n_samples=2000)\n</pre> import pandas as pd import matplotlib.pyplot as plt from ssms.basic_simulators.simulator import simulator  data = pd.DataFrame(     np.zeros((2000, 7), dtype=np.float32),     columns=[\"v\", \"a\", \"z\", \"t\", \"theta\", \"rt\", \"choice\"], ) data[\"v\"] = 0.5 data[\"a\"] = 0.75 data[\"z\"] = 0.5 data[\"t\"] = 0.2 data[\"theta\"] = 0.1 data.loc[np.arange(0, 1000), \"rt\"] = np.linspace(5, 0, 1000).astype(np.float32) data.loc[np.arange(1000, 2000), \"rt\"] = np.linspace(0, 5, 1000).astype(np.float32) data.loc[np.arange(0, 1000), \"choice\"] = -1 data.loc[np.arange(1000, 2000), \"choice\"] = 1  # Network predictions predict_on_batch_out = network.predict_on_batch(data.values.astype(np.float32))  # Simulations sim_out = simulator(model=\"angle\", theta=data.values[0, :-2], n_samples=2000) In\u00a0[56]: Copied! <pre># Plot network predictions\nplt.plot(\n    data[\"rt\"] * data[\"choice\"],\n    np.exp(predict_on_batch_out),\n    color=\"black\",\n    label=\"network\",\n)\n\n# Plot simulations\nplt.hist(\n    sim_out[\"rts\"] * sim_out[\"choices\"],\n    bins=100,\n    histtype=\"step\",\n    label=\"simulations\",\n    color=\"blue\",\n    density=True,\n)\nplt.legend()\nplt.title(\"SSM likelihood\")\nplt.xlabel(\"rt\")\nplt.ylabel(\"likelihod\")\n</pre> # Plot network predictions plt.plot(     data[\"rt\"] * data[\"choice\"],     np.exp(predict_on_batch_out),     color=\"black\",     label=\"network\", )  # Plot simulations plt.hist(     sim_out[\"rts\"] * sim_out[\"choices\"],     bins=100,     histtype=\"step\",     label=\"simulations\",     color=\"blue\",     density=True, ) plt.legend() plt.title(\"SSM likelihood\") plt.xlabel(\"rt\") plt.ylabel(\"likelihod\") Out[56]: <pre>Text(0, 0.5, 'likelihod')</pre> In\u00a0[57]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom ssms.basic_simulators.simulator import simulator\n\ndata = pd.DataFrame(\n    np.zeros((2000, 7), dtype=np.float32),\n    columns=[\"v\", \"a\", \"z\", \"t\", \"theta\", \"rt\", \"choice\"],\n)\ndata[\"v\"] = 0.5\ndata[\"a\"] = 0.75\ndata[\"z\"] = 0.5\ndata[\"t\"] = 0.2\ndata[\"theta\"] = 0.1\ndata.loc[np.arange(0, 1000), \"rt\"] = np.linspace(20, 0, 1000).astype(np.float32)\ndata.loc[np.arange(1000, 2000), \"rt\"] = np.linspace(0, 20, 1000).astype(np.float32)\ndata.loc[np.arange(0, 1000), \"choice\"] = -1\ndata.loc[np.arange(1000, 2000), \"choice\"] = 1\n\n# Network predictions\npredict_on_batch_out = network.predict_on_batch(data.values.astype(np.float32))\n\n# Simulations\nsim_out = simulator(model=\"angle\", theta=data.values[0, :-2], n_samples=2000)\n</pre> import pandas as pd import matplotlib.pyplot as plt from ssms.basic_simulators.simulator import simulator  data = pd.DataFrame(     np.zeros((2000, 7), dtype=np.float32),     columns=[\"v\", \"a\", \"z\", \"t\", \"theta\", \"rt\", \"choice\"], ) data[\"v\"] = 0.5 data[\"a\"] = 0.75 data[\"z\"] = 0.5 data[\"t\"] = 0.2 data[\"theta\"] = 0.1 data.loc[np.arange(0, 1000), \"rt\"] = np.linspace(20, 0, 1000).astype(np.float32) data.loc[np.arange(1000, 2000), \"rt\"] = np.linspace(0, 20, 1000).astype(np.float32) data.loc[np.arange(0, 1000), \"choice\"] = -1 data.loc[np.arange(1000, 2000), \"choice\"] = 1  # Network predictions predict_on_batch_out = network.predict_on_batch(data.values.astype(np.float32))  # Simulations sim_out = simulator(model=\"angle\", theta=data.values[0, :-2], n_samples=2000) In\u00a0[59]: Copied! <pre># Plot network predictions\nplt.plot(\n    data[\"rt\"] * data[\"choice\"],\n    predict_on_batch_out,\n    color=\"black\",\n    label=\"network\",\n)\n\nplt.legend()\nplt.title(\"SSM log-likelihood\")\nplt.xlabel(\"rt\")\nplt.ylabel(\"logl\")\n</pre> # Plot network predictions plt.plot(     data[\"rt\"] * data[\"choice\"],     predict_on_batch_out,     color=\"black\",     label=\"network\", )  plt.legend() plt.title(\"SSM log-likelihood\") plt.xlabel(\"rt\") plt.ylabel(\"logl\") Out[59]: <pre>Text(0, 0.5, 'logl')</pre> <p>We hope this package may be helpful in case you attempt to train LANs for your own research.</p>"},{"location":"basic_tutorial/basic_tutorial/#quick-start","title":"Quick Start\u00b6","text":""},{"location":"basic_tutorial/basic_tutorial/#install","title":"Install\u00b6","text":"<p>To install the <code>ssms</code> package type,</p> <p><code>pip install git+https://github.com/AlexanderFengler/ssm_simulators</code></p> <p>To install the <code>LANfactory</code> package type,</p> <p><code>pip install git+https://github.com/AlexanderFengler/LANfactory</code></p> <p>Necessary dependency should be installed automatically in the process.</p>"},{"location":"basic_tutorial/basic_tutorial/#basic-tutorial","title":"Basic Tutorial\u00b6","text":""},{"location":"basic_tutorial/basic_tutorial/#generate-training-data","title":"Generate Training Data\u00b6","text":"<p>First we need to generate some training data. As mentioned above we will do so using the <code>ssms</code> python package, however without delving into a detailed explanation of this package. Please refer to the [basic ssms tutorial] (https://github.com/AlexanderFengler/ssm_simulators) in case you want to learn more.</p>"},{"location":"basic_tutorial/basic_tutorial/#prepare-for-training","title":"Prepare for Training\u00b6","text":"<p>Next we set up dataloaders for training with pytorch. The <code>LANfactory</code> uses custom dataloaders, taking into account particularities of the expected training data. Specifically, we expect to receive a bunch of training data files (the present example generates only one), where each file hosts a large number of training examples. So we want to define a dataloader which spits out batches from data with a specific training data file, and keeps checking when to load in a new file. The way this is implemented here, is via the <code>DatasetTorch</code> class in <code>lanfactory.trainers</code>, which inherits from <code>torch.utils.data.Dataset</code> and prespecifies a <code>batch_size</code>. Finally this is supplied to a <code>DataLoader</code>, for which we keep the <code>batch_size</code> argument at 0.</p> <p>The <code>DatasetTorch</code> class is then called as an iterator via the DataLoader and takes care of batching as well as file loading internally.</p> <p>You may choose your own way of defining the <code>DataLoader</code> classes, downstream you are simply expected to supply one.</p>"},{"location":"basic_tutorial/basic_tutorial/#load-model-for-inference-and-call","title":"Load Model for Inference and Call\u00b6","text":""},{"location":"basic_tutorial/basic_tutorial/#a-peek-into-the-first-passage-distribution-computed-by-the-network","title":"A peek into the first passage distribution computed by the network\u00b6","text":"<p>We can compare the learned likelihood function in our <code>network</code> with simulation data from the underlying generative model. For this purpose we recruit the <code>ssms</code> package again.</p>"},{"location":"basic_tutorial/basic_tutorial/#end","title":"END\u00b6","text":""}]}